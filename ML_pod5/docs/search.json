[
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Using Python Virtual Environments (venv) in VS Code",
    "section": "",
    "text": "A virtual environment (venv) isolates a project’s Python packages so different projects can use different versions without conflicts. This guide shows how\n\nto create a venv,\ninstall packages,\nand configure VS Code to use it on macOS/Linux and Windows.",
    "crumbs": [
      "Set-up",
      "Using Python Virtual Environments (venv) in VS Code"
    ]
  },
  {
    "objectID": "setup.html#overview",
    "href": "setup.html#overview",
    "title": "Using Python Virtual Environments (venv) in VS Code",
    "section": "",
    "text": "A virtual environment (venv) isolates a project’s Python packages so different projects can use different versions without conflicts. This guide shows how\n\nto create a venv,\ninstall packages,\nand configure VS Code to use it on macOS/Linux and Windows.",
    "crumbs": [
      "Set-up",
      "Using Python Virtual Environments (venv) in VS Code"
    ]
  },
  {
    "objectID": "setup.html#why-environments-matter-in-python",
    "href": "setup.html#why-environments-matter-in-python",
    "title": "Using Python Virtual Environments (venv) in VS Code",
    "section": "Why environments matter in Python",
    "text": "Why environments matter in Python\n\n\n\n\n\n\nTip\n\n\n\nEnvironments keep your work reproducible, conflict-free, and safe.\n\n\nPython installs packages globally by default. Over time, this can create dependency conflicts—for example:\n\nProject A needs NumPy 1.22, while Project B needs NumPy 2.0.\n\nInstalling new libraries for one project breaks another.\n\nReproducing your setup on another computer (or on a cluster) becomes difficult.\n\nA virtual environment solves this by giving each project its own isolated package space:\n\nYou can install, upgrade, or remove packages without affecting other projects.\n\nYour requirements.txt or environment.yml fully defines what’s needed.\n\nColleagues (or future you) can recreate the same environment anywhere.\n\nThink of it as a sandboxed workspace for each project.\n\n\n\n\n\n\nTip\n\n\n\nWhen to use venv vs conda?\nIf you don’t need compiled scientific stacks managed by conda, venv + pip is lightweight and portable. You can always switch to conda later if needed.",
    "crumbs": [
      "Set-up",
      "Using Python Virtual Environments (venv) in VS Code"
    ]
  },
  {
    "objectID": "setup.html#prerequisites",
    "href": "setup.html#prerequisites",
    "title": "Using Python Virtual Environments (venv) in VS Code",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nPython 3.10+ (latest version is 3.13) installed and available on your PATH (python --version).\nVS Code with the Python and Jupyter extensions.\nBasic terminal (macOS/Linux) or PowerShell (Windows) access.",
    "crumbs": [
      "Set-up",
      "Using Python Virtual Environments (venv) in VS Code"
    ]
  },
  {
    "objectID": "setup.html#create-a-virtual-environment",
    "href": "setup.html#create-a-virtual-environment",
    "title": "Using Python Virtual Environments (venv) in VS Code",
    "section": "1. Create a virtual environment",
    "text": "1. Create a virtual environment\n\nmacOS / Linux\n# in your project folder\npython -m venv .venv\n\n# activate it\nsource .venv/bin/activate\n# your prompt should show (.venv)\n\n\nWindows powershell:\n# in your project folder\npython -m venv .venv\n\n# activate it\n.\\.venv\\Scripts\\Activate.ps1\n# your prompt should show (.venv)\nYou should be able to see a folder .venv in your project folder.\n\n\n\n\n\n\nNote\n\n\n\nThe folder name .venv is conventional and recognized by VS Code automatically if it lives in the project root.",
    "crumbs": [
      "Set-up",
      "Using Python Virtual Environments (venv) in VS Code"
    ]
  },
  {
    "objectID": "setup.html#install-your-project-packages",
    "href": "setup.html#install-your-project-packages",
    "title": "Using Python Virtual Environments (venv) in VS Code",
    "section": "2. Install your project packages",
    "text": "2. Install your project packages\nWith the environment activated, install what you need in the terminal. The main installer in python is pip, while other, newer (better) options do exist (uv, conda).\npip install numpy pandas matplotlib scikit-learn jupyter",
    "crumbs": [
      "Set-up",
      "Using Python Virtual Environments (venv) in VS Code"
    ]
  },
  {
    "objectID": "setup.html#open-the-project-in-vs-code",
    "href": "setup.html#open-the-project-in-vs-code",
    "title": "Using Python Virtual Environments (venv) in VS Code",
    "section": "3. Open the project in VS Code",
    "text": "3. Open the project in VS Code\n\nFile → Open Folder… and select your project folder (the one containing .venv).\nVS Code usually detects the environment. If not, select it manually (next section).",
    "crumbs": [
      "Set-up",
      "Using Python Virtual Environments (venv) in VS Code"
    ]
  },
  {
    "objectID": "setup.html#select-the-interpreter-in-vs-code",
    "href": "setup.html#select-the-interpreter-in-vs-code",
    "title": "Using Python Virtual Environments (venv) in VS Code",
    "section": "4. Select the interpreter in VS Code",
    "text": "4. Select the interpreter in VS Code\n1 Press ⌘⇧P / Ctrl+Shift+P → Python: Select Interpreter\n2.  Choose your environment path:\n./.venv/bin/python           # macOS/Linux\n.\\.venv\\Scripts\\python.exe   # Windows",
    "crumbs": [
      "Set-up",
      "Using Python Virtual Environments (venv) in VS Code"
    ]
  },
  {
    "objectID": "setup.html#jupyter-notebooks-optional",
    "href": "setup.html#jupyter-notebooks-optional",
    "title": "Using Python Virtual Environments (venv) in VS Code",
    "section": "5. Jupyter & notebooks (optional)",
    "text": "5. Jupyter & notebooks (optional)\nWhen you create or open a notebook (.ipynb), click the kernel picker (top-right) and select your .venv interpreter. Select your .venv interpreter",
    "crumbs": [
      "Set-up",
      "Using Python Virtual Environments (venv) in VS Code"
    ]
  },
  {
    "objectID": "setup.html#test-your-environment.",
    "href": "setup.html#test-your-environment.",
    "title": "Using Python Virtual Environments (venv) in VS Code",
    "section": "6 Test your environment.",
    "text": "6 Test your environment.\n\nActivate your environment in your terminal. Install the numpy package\n\npip install numpy\n\nCreate a file test.py. Import the numpy package\n\nimport numpy as np",
    "crumbs": [
      "Set-up",
      "Using Python Virtual Environments (venv) in VS Code"
    ]
  },
  {
    "objectID": "basic-jupyter.html",
    "href": "basic-jupyter.html",
    "title": "Jupyter Notebooks in Google Colab",
    "section": "",
    "text": "Alternatively, if you want to use Python without worrying about installing libraries or managing dependencies, you can use Jupyter notebooks and run them directly in Google Colab — a free, cloud-based platform provided by Google.",
    "crumbs": [
      "Set-up",
      "Jupyter Notebooks in Google Colab"
    ]
  },
  {
    "objectID": "basic-jupyter.html#what-is-google-colab",
    "href": "basic-jupyter.html#what-is-google-colab",
    "title": "Jupyter Notebooks in Google Colab",
    "section": "1. What is Google Colab?",
    "text": "1. What is Google Colab?\nGoogle Colab (short for Collaboratory) is a cloud service that lets you run Jupyter notebooks in your browser, with no local setup required.\nKey advantages: - No need to install Python or libraries. - Access to free GPUs and TPUs for computation. - Direct integration with Google Drive for storage. - Same syntax and workflow as local Jupyter notebooks (.ipynb files).\nYou can think of it as “Jupyter Notebook in the cloud.”",
    "crumbs": [
      "Set-up",
      "Jupyter Notebooks in Google Colab"
    ]
  },
  {
    "objectID": "basic-jupyter.html#how-to-open-a-notebook-in-colab",
    "href": "basic-jupyter.html#how-to-open-a-notebook-in-colab",
    "title": "Jupyter Notebooks in Google Colab",
    "section": "2. How to open a notebook in Colab",
    "text": "2. How to open a notebook in Colab\n\nOption A – Start from scratch\n\nGo to https://colab.research.google.com\n\nChoose File → New notebook.\n\nA notebook with a Python 3 runtime will open in your browser.\n\nYou can rename it and save it to your Google Drive.\n\n\n\nOption B – Open an existing notebook\nIf you already have a .ipynb file:\n\nUpload it to Google Drive.\n\nRight-click → Open with → Google Colaboratory.\n(If Colab isn’t listed, choose Connect more apps → search for Colaboratory.)\n\nAlternatively, you can drag-and-drop a notebook directly into an open Colab window.",
    "crumbs": [
      "Set-up",
      "Jupyter Notebooks in Google Colab"
    ]
  },
  {
    "objectID": "basic-jupyter.html#installing-additional-libraries",
    "href": "basic-jupyter.html#installing-additional-libraries",
    "title": "Jupyter Notebooks in Google Colab",
    "section": "3. Installing additional libraries",
    "text": "3. Installing additional libraries\nColab comes pre-installed with most scientific packages (NumPy, pandas, matplotlib, scikit-learn, PyTorch, TensorFlow, etc.).\nTo install any additional packages, use pip directly in a cell:\n!pip install seaborn xgboost shap",
    "crumbs": [
      "Set-up",
      "Jupyter Notebooks in Google Colab"
    ]
  },
  {
    "objectID": "task_setup.html",
    "href": "task_setup.html",
    "title": "Task: Setting up VS Code for Python",
    "section": "",
    "text": "Set up and test your Python development environment in VS Code using a virtual environment.",
    "crumbs": [
      "Set-up",
      "Task: Setting up VS Code for Python"
    ]
  },
  {
    "objectID": "task_setup.html#objective",
    "href": "task_setup.html#objective",
    "title": "Task: Setting up VS Code for Python",
    "section": "",
    "text": "Set up and test your Python development environment in VS Code using a virtual environment.",
    "crumbs": [
      "Set-up",
      "Task: Setting up VS Code for Python"
    ]
  },
  {
    "objectID": "task_setup.html#step-by-step-instructions",
    "href": "task_setup.html#step-by-step-instructions",
    "title": "Task: Setting up VS Code for Python",
    "section": "Step-by-step instructions",
    "text": "Step-by-step instructions\n\n1. Check your Python installation\nOpen your terminal and verify that Python is installed: bash    python3 --version\n\n\n2. Install VS Code extensions\nOpen VS Code and install the following: • Python extension • Jupyter extension\n\n\n3. Create and activate a virtual environment\nIn the terminal (inside your project folder), run:\npython3 -m venv .venv\nsource .venv/bin/activate       # macOS/Linux\nOR\n.\\.venv\\Scripts\\Activate.ps1    # Windows\nThen install a few essential packages:\npip install jupyter numpy scikit-learn matplotlib pandas\n\n\n4. Test Python and NumPy in the terminal\nLaunch Python from within your activated environment and check:\nimport numpy as np\nprint(np.__version__)\n\n\n5. Test in VS Code using a Python script\n•   Create a new file called test.py\n•   Add the following code:\nimport numpy as np\nprint(\"NumPy works!\", np.__version__)\n\n\n6. Test in a Jupyter Notebook\n•   Create a new notebook named test.ipynb\n•   In the first cell, add:\nimport numpy as np\nnp.random.rand(3)\n• Run the cell and confirm it executes successfully.\n\n\n7. (Optional) Try running in Google Colab\nUpload your test.ipynb to Google Colab. In the first cell, install NumPy and run:\n!pip install numpy\nimport numpy as np\nprint(\"NumPy works in Colab!\", np.__version__)",
    "crumbs": [
      "Set-up",
      "Task: Setting up VS Code for Python"
    ]
  },
  {
    "objectID": "types-of-ml.html",
    "href": "types-of-ml.html",
    "title": "Types of Machine learning Algorithms",
    "section": "",
    "text": "Use when: target is numeric (e.g., gait speed, length of stay, brain-age delta).\n\nLinear Regression — baseline linear fit; fast, interpretable.\n\nRidge (L2) — shrinks coefficients; helps multicollinearity.\n\nLasso (L1) — feature selection via sparsity.\n\nElastic Net — hybrid of L1/L2; robust when features are correlated.\n\nGeneralized Linear Models (GLM) — non-Gaussian outcomes (Poisson, Gamma).\n\nSupport Vector Regression (SVR) — robust, kernelized nonlinearity.\n\nTree Ensembles (RF/GBM) — capture nonlinearity & interactions.\n\nUse-cases: predict 6MWT, therapy hours, WMH volume change, LOS.\n\n\n\nUse when: target is categorical (e.g., fall/no-fall, discharge home vs facility).\n\nLogistic Regression — strong baseline, well-calibrated.\nNaive Bayes — simple probabilistic baseline (text, high-dim data).\nk-Nearest Neighbors (kNN) — instance-based, local decision boundaries.\nDecision Trees / Random Forests — interpretable (trees) / strong default (RF).\nGradient Boosted Trees — XGBoost / LightGBM / CatBoost; top tabular performers.\nSupport Vector Machines (SVM) — good in high-dimensional settings.\n\nUse-cases: classify stroke discharge, fall risk from IMU, diagnosis subtypes."
  },
  {
    "objectID": "types-of-ml.html#supervised-learning",
    "href": "types-of-ml.html#supervised-learning",
    "title": "Types of Machine learning Algorithms",
    "section": "",
    "text": "Use when: target is numeric (e.g., gait speed, length of stay, brain-age delta).\n\nLinear Regression — baseline linear fit; fast, interpretable.\n\nRidge (L2) — shrinks coefficients; helps multicollinearity.\n\nLasso (L1) — feature selection via sparsity.\n\nElastic Net — hybrid of L1/L2; robust when features are correlated.\n\nGeneralized Linear Models (GLM) — non-Gaussian outcomes (Poisson, Gamma).\n\nSupport Vector Regression (SVR) — robust, kernelized nonlinearity.\n\nTree Ensembles (RF/GBM) — capture nonlinearity & interactions.\n\nUse-cases: predict 6MWT, therapy hours, WMH volume change, LOS.\n\n\n\nUse when: target is categorical (e.g., fall/no-fall, discharge home vs facility).\n\nLogistic Regression — strong baseline, well-calibrated.\nNaive Bayes — simple probabilistic baseline (text, high-dim data).\nk-Nearest Neighbors (kNN) — instance-based, local decision boundaries.\nDecision Trees / Random Forests — interpretable (trees) / strong default (RF).\nGradient Boosted Trees — XGBoost / LightGBM / CatBoost; top tabular performers.\nSupport Vector Machines (SVM) — good in high-dimensional settings.\n\nUse-cases: classify stroke discharge, fall risk from IMU, diagnosis subtypes."
  },
  {
    "objectID": "types-of-ml.html#unsupervised-learning",
    "href": "types-of-ml.html#unsupervised-learning",
    "title": "Types of Machine learning Algorithms",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\n\nClustering (find structure in unlabeled data)\n\nk-Means — spherical clusters; fast; needs k.\nHierarchical clustering — dendrogram; no fixed k.\nDBSCAN — density-based; finds arbitrary shapes; flags noise.\nGaussian Mixture Models (GMM) — soft assignments; probabilistic.\n\nUse-cases: patient subtyping, gait pattern families, sensor-based phenotypes."
  },
  {
    "objectID": "types-of-ml.html#dimensionality-reduction-feature-extraction",
    "href": "types-of-ml.html#dimensionality-reduction-feature-extraction",
    "title": "Types of Machine learning Algorithms",
    "section": "Dimensionality reduction / Feature extraction",
    "text": "Dimensionality reduction / Feature extraction\n\nPCA — orthogonal components maximizing variance.\nICA — source separation (EEG/fMRI).\nt-SNE / UMAP — nonlinear embeddings (visualization).\nAutoencoders — learned low-dim representations.\nFeature selection — Lasso, RFE, mutual information.\n\nUse-cases: reduce voxel/ROI features, compress IMU time-series features."
  },
  {
    "objectID": "types-of-ml.html#anomaly-novelty-detection",
    "href": "types-of-ml.html#anomaly-novelty-detection",
    "title": "Types of Machine learning Algorithms",
    "section": "Anomaly / Novelty detection",
    "text": "Anomaly / Novelty detection\n\nGaussian Mixtures — density modeling; low-probability = anomaly.\nOne-Class SVM — boundary around “normal”.\nIsolation Forest — isolates rare points via random splits.\nLocal Outlier Factor (LOF) — local density deviations.\n\nUse-cases: flag sensor glitches, unusual recovery trajectories, QC in pipelines."
  },
  {
    "objectID": "types-of-ml.html#probabilistic-time-series",
    "href": "types-of-ml.html#probabilistic-time-series",
    "title": "Types of Machine learning Algorithms",
    "section": "Probabilistic & time-series",
    "text": "Probabilistic & time-series\n\nHidden Markov Models (HMM) — discrete latent states for sequences (e.g., gait phases).\nGaussian Processes (GP) — Bayesian nonparametric regression/classification with uncertainty.\nBayesian Networks — causal/conditional dependencies among variables.\n\nUse-cases: session-to-session recovery trajectories, gait state decoding, uncertainty."
  },
  {
    "objectID": "types-of-ml.html#ensembles",
    "href": "types-of-ml.html#ensembles",
    "title": "Types of Machine learning Algorithms",
    "section": "Ensembles",
    "text": "Ensembles\nEnsemble methods\n\nBagging — reduces variance (e.g., Random Forest).\nBoosting — reduces bias with additive trees (e.g., XGBoost, LightGBM, CatBoost).\nStacking — meta-learner combining diverse base models.\n\nUse-cases: robust tabular prediction with mixed clinical + sensor features."
  },
  {
    "objectID": "types-of-ml.html#optional-deep-learning",
    "href": "types-of-ml.html#optional-deep-learning",
    "title": "Types of Machine learning Algorithms",
    "section": "(Optional) Deep learning",
    "text": "(Optional) Deep learning\nNeural networks (when data are large / structured)\n\nMLP — general nonlinear function approximator for tabular + small signals.\nCNN — spatial/temporal patterns (images, spectrograms, kinematic grids).\nRNN/GRU/LSTM — sequence modeling (time-series).\nTransformers — state-of-the-art for sequences; scalable attention.\n\nUse-cases: MRI/US imaging, multi-sensor sequences, language notes (NLP)."
  },
  {
    "objectID": "types-of-ml.html#choosing-a-model-fast-decision-guide",
    "href": "types-of-ml.html#choosing-a-model-fast-decision-guide",
    "title": "Types of Machine learning Algorithms",
    "section": "Choosing a model (fast decision guide)",
    "text": "Choosing a model (fast decision guide)\n\nSmall/medium tabular data: start with Logistic/Linear, Random Forest, Gradient Boosting.\n\nHigh-dimensional & few samples: try Lasso/Elastic Net, SVM, RF (use strong CV).\n\nUnlabeled exploration: PCA/UMAP + clustering (DBSCAN/GMM).\n\nTime-series / stateful: HMM, RNN/Transformer (if data are sufficient).\n\nNeed calibrated probabilities: Logistic Regression, CalibratedClassifierCV.\n\n\n\n\n\n\n\nWarning\n\n\n\nValidation matters more than model choice.\nUse stratified/nested CV, GroupKFold for repeated patients, temporal splits for prognosis, and check calibration + subgroup fairness."
  },
  {
    "objectID": "types-of-ml.html#key-sklearn-entry-points",
    "href": "types-of-ml.html#key-sklearn-entry-points",
    "title": "Types of Machine learning Algorithms",
    "section": "Key sklearn entry points",
    "text": "Key sklearn entry points\n\nLinearRegression, Ridge, Lasso, ElasticNet, PoissonRegressor\n\nLogisticRegression, SVC/SVR, KNeighbors*, GaussianNB\n\nDecisionTree*, RandomForest*, GradientBoosting*\n\nKMeans, AgglomerativeClustering, DBSCAN, GaussianMixture\n\nPCA, TruncatedSVD, FactorAnalysis, SelectKBest, RFE\n\nOneClassSVM, IsolationForest, LocalOutlierFactor\n\nPipelines & Validation: Pipeline, ColumnTransformer, StratifiedKFold, GroupKFold, GridSearchCV, CalibratedClassifierCV"
  },
  {
    "objectID": "types-of-ml.html#further-reading",
    "href": "types-of-ml.html#further-reading",
    "title": "Types of Machine learning Algorithms",
    "section": "Further reading",
    "text": "Further reading\n\nscikit-learn User Guide (models, pipelines, model evaluation)\n\nElements of Statistical Learning (Hastie, Tibshirani, Friedman)\n\nInterpretable ML (Molnar) — SHAP, partial dependence, global vs local explanations"
  },
  {
    "objectID": "Regression_from_scratch.html",
    "href": "Regression_from_scratch.html",
    "title": "Regression from scratch",
    "section": "",
    "text": "Symbol Type\nExample\nTypical Shape\nDescription\n\n\n\n\nScalar\n\\(a, b, x_i, y, \\eta\\)\n\\(1 \\times 1\\)\nSingle numeric value (e.g., a feature value, bias, or loss).\n\n\nVector\n\\(\\mathbf{x}, \\mathbf{w}, \\boldsymbol{\\theta}\\)\n\\(p \\times 1\\)\nColumn vector containing multiple values (e.g., features or parameters).\n\n\nMatrix\n\\(\\mathbf{X}, \\mathbf{W}, \\mathbf{A}\\)\n\\(n \\times p\\)\n2-D array (rows = samples, columns = features).\n\n\nDot product\n\\(\\mathbf{w}^T \\mathbf{x}\\)\nscalar\nInner product between two vectors.\n\n\nOuter product\n\\(\\mathbf{x}\\mathbf{w}^T\\)\n\\(p \\times p\\)\nCreates a matrix from two vectors.\n\n\nInverse\n\\(\\mathbf{A}^{-1}\\)\n\\(p \\times p\\)\nMatrix that “undoes” multiplication by \\(\\mathbf{A}\\), if it exists.\n\n\nHat notation\n\\(\\hat{y}\\)\nscalar or vector\nEstimated or predicted value (e.g., \\(\\hat{y}\\) = model prediction).\n\n\nBar notation\n\\(\\bar{x}\\)\nscalar or vector\nMean or average value (e.g., \\(\\bar{x}\\) = sample mean).",
    "crumbs": [
      "What is machine learning?",
      "Regression from scratch"
    ]
  },
  {
    "objectID": "Regression_from_scratch.html#the-model",
    "href": "Regression_from_scratch.html#the-model",
    "title": "Regression from scratch",
    "section": "1. The model",
    "text": "1. The model\nThe mathematical structure or function family that maps inputs to outputs (supervised learning). The model defines what forms of relationships between input and output can be learned.\n\nFor linear regression:\n\n\n1. Scalar form\nEach observation (i) has its own equation: \\[\n\\hat{y}_i = w_0 + w_1 x_{i1} + w_2 x_{i2} + \\dots + w_p x_{ip} + \\varepsilon_i\n\\]\n\n\n2. Vector (standard math) form\nCompact form for a single observation using vectors: \\[\n\\hat{y}_i = \\mathbf{w}^T \\mathbf{x}_i + b + \\varepsilon_i\n\\]\n\n\n3. Matrix form\nAll (n) observations combined (X stacks row vectors): \\[\n\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w} + \\mathbf{b} + \\boldsymbol{\\varepsilon}\n\\] or (if (b) is absorbed as an intercept column of ones in ()): \\[\n\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w} + \\boldsymbol{\\varepsilon}\n\\]",
    "crumbs": [
      "What is machine learning?",
      "Regression from scratch"
    ]
  },
  {
    "objectID": "Regression_from_scratch.html#input-data",
    "href": "Regression_from_scratch.html#input-data",
    "title": "Regression from scratch",
    "section": "2. Input Data",
    "text": "2. Input Data\n\nExamples: images, text, sensor data, neuroimaging features, etc.\nOften represented as a matrix X of shape \\((n_{samples}, n_{features})\\).\nData needs to be cleaned, normalized, encoded, and sometimes split into train/test sets.\n\n\nThe target variable Y (what should be predicted)\n\\[\nY =\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\ny_3\n\\end{bmatrix}\n\\]\n\n\nThe input data (what predicts)\n\\[\nX =\n\\begin{bmatrix}\n1 & \\text{Age}_1 & \\text{Sex}_1 \\\\\n1 & \\text{Age}_2 & \\text{Sex}_2 \\\\\n1 & \\text{Age}_3 & \\text{Sex}_3\n\\end{bmatrix}\n\\]\n\n\n3. Parameters (Weights / Coefficients)\nThe tunable variables that define a specific instance of the model.\nExamples:\n\nw, b in linear regression\nConnection weights in a neural network\nDuring learning, these parameters are adjusted to best fit the data\n\n\\[\nw =\n\\begin{bmatrix}\nw_1 \\\\\nw_2 \\\\\nw_3\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "What is machine learning?",
      "Regression from scratch"
    ]
  },
  {
    "objectID": "Regression_from_scratch.html#detour-matrix-multiplication",
    "href": "Regression_from_scratch.html#detour-matrix-multiplication",
    "title": "Regression from scratch",
    "section": "Detour: Matrix multiplication",
    "text": "Detour: Matrix multiplication\nMatrix multiplication is one of the most common operations in linear algebra.\nWhen we multiply two matrices, the number of columns in the first must match the number of rows in the second.\nIf \\(\\mathbf{A}\\) is of shape \\(m \\times n\\) and \\(\\mathbf{B}\\) is of shape \\(n \\times p\\),\nthen the result \\(\\mathbf{C} = \\mathbf{A}\\mathbf{B}\\) will have shape \\(m \\times p\\).\nEach element of the result is the dot product of a row from \\(\\mathbf{A}\\) and a column from \\(\\mathbf{B}\\):\n\\[\nc_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj}\n\\]\nAs an example, here is the multiplication of a 2 x 3 by a 3 x 2 matrix, resulting in a 2 x 2 matrix:\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w} + \\mathbf{b} + \\boldsymbol{\\varepsilon} =\n\\]\n\\[\n=\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23}\n\\end{bmatrix}\n\\begin{bmatrix}\nb_{11} & b_{12} \\\\\nb_{21} & b_{22} \\\\\nb_{31} & b_{32}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\na_{11}b_{11} + a_{12}b_{21} + a_{13}b_{31} & a_{11}b_{12} + a_{12}b_{22} + a_{13}b_{32} \\\\\na_{21}b_{11} + a_{22}b_{21} + a_{23}b_{31} & a_{21}b_{12} + a_{22}b_{22} + a_{23}b_{32}\n\\end{bmatrix}\n\\]\nExercise:\n\nExercise: Linear Regression Prediction\nA linear regression model estimates the relationship between predictors (features) and a target variable.\nGiven an input matrix \\(\\mathbf{X}\\) and a weight vector \\(\\mathbf{w}\\), the model predicts target values \\(\\hat{\\mathbf{y}}\\) according to:\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w}\n\\]\n\nTask\nYou are given the following data X, that the following weights w were fitted to:\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n1 & 2 \\\\\n1 & 4 \\\\\n1 & 6\n\\end{bmatrix},\n\\quad\n\\mathbf{w} =\n\\begin{bmatrix}\n0.5 \\\\\n1.2\n\\end{bmatrix}\n\\]\n\nCompute the predicted values \\(\\hat{\\mathbf{y}}\\) (by hand).\nDo the same using the numpy package using the dot product:\n\nimport numpy as np\nX = np.array([[1,2],[1,4],[1,6]])\nw = np.array([[0.5],[1.2]])\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGoal: Compute \\(\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w}\\).\nGiven \\[\n\\mathbf{X} =\n\\begin{bmatrix}\n1 & 2 \\\\\n1 & 4 \\\\\n1 & 6\n\\end{bmatrix},\n\\quad\n\\mathbf{w} =\n\\begin{bmatrix}\n0.5 \\\\\n1.2\n\\end{bmatrix}\n\\]\nStep-by-step \\[\n\\hat{\\mathbf{y}} =\n\\begin{bmatrix}\n1 & 2 \\\\\n1 & 4 \\\\\n1 & 6\n\\end{bmatrix}\n\\begin{bmatrix}\n0.5 \\\\\n1.2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1\\cdot0.5 + 2\\cdot1.2 \\\\\n1\\cdot0.5 + 4\\cdot1.2 \\\\\n1\\cdot0.5 + 6\\cdot1.2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2.9 \\\\\n5.3 \\\\\n7.7\n\\end{bmatrix}\n\\]\nResult \\[\n\\hat{\\mathbf{y}} =\n\\begin{bmatrix}\n2.9 \\\\\n5.3 \\\\\n7.7\n\\end{bmatrix}\n\\]\n** Using NumPy check**\nimport numpy as np\nX = np.array([[1,2],[1,4],[1,6]])\nw = np.array([[0.5],[1.2]])\n#y_hat = X @ w\nnp.dot(X,w)\ny_hat\n\n\n\n\n\n\n4. Objective or Loss Function\nA quantitative measure of how well the model performs.\n\nIt defines the goal of learning.\nExamples:\n\nMean Squared Error (MSE) for regression\nCross-Entropy Loss for classification\nNegative log-likelihood for probabilistic models\n\n\nAim: The algorithm tries to minimize (or maximize) this loss function.\nExample: mean squared loss \\[\nL = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2\n\\]\n\\(L\\) — total loss (the value we minimize)\n\\(n\\) — number of samples\n\\(y_i\\) — true (observed) value for sample i\n\\(\\hat{y}_i\\) — predicted value for sample i\nThe squared difference \\((\\hat{y}_i - y_i)^2\\) measures the error for each prediction. Taking the mean makes the loss independent of sample size.\n\nTask\nYou are given a target vector \\(y\\), in addition to the previous vector \\(\\hat{y}\\):\n\\[\n\\mathbf{y} =\n\\begin{bmatrix}\n3.0 \\\\\n5.0 \\\\\n8.0\n\\end{bmatrix},\n\\qquad\n\\hat{\\mathbf{y}} =\n\\begin{bmatrix}\n2.9 \\\\\n5.3 \\\\\n7.7\n\\end{bmatrix}\n\\]\n\nCalculate the mean square loss between the predicted changes from the previous task and the target vector y by hand.\nWrite a function in python to do it. The function should take \\(y\\) and \\(\\hat{y}\\) as input and return the mean squared loss:\n\nLoss = mean_quared_loss(y, y_hat)\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can compute the Mean Squared Error (MSE) step by step.\nGiven\n\\[\n\\mathbf{y} =\n\\begin{bmatrix}\n3.0 \\\\\n5.0 \\\\\n8.0\n\\end{bmatrix},\n\\qquad\n\\hat{\\mathbf{y}} =\n\\begin{bmatrix}\n2.9 \\\\\n5.3 \\\\\n7.7\n\\end{bmatrix}\n\\]\n\n\nStep 1 — Write the formula\n\\[\nL = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2\n\\]\nHere \\(n = 3\\).\n\n\n\nStep 2 — Compute the individual errors\n\n\n\n\n\n\n\n\n\n\n(i)\n(y_i)\n(_i)\n(_i - y_i)\n((_i - y_i)^2)\n\n\n\n\n1\n3.0\n2.9\n-0.1\n0.01\n\n\n2\n5.0\n5.3\n+0.3\n0.09\n\n\n3\n8.0\n7.7\n-0.3\n0.09\n\n\n\n\n\n\nStep 3 — Sum and average\n\\[\nL = \\frac{1}{3} (0.01 + 0.09 + 0.09)\n= \\frac{0.19}{3}\n= 0.0633\n\\]\nFinal Result \\[\nL = 0.0633\n\\]\n\n\n\n\n\n\n\n\n\n\n\nSolution Python\n\n\n\n\n\nimport numpy as np\n\ny = np.array([3.0, 5.0, 8.0])\ny_hat = np.array([2.9, 5.3, 7.7])\n\ndef mse(y, y_hat):\n    result = np.mean((y_hat - y)**2)\n    return result\n\n\n\n\n\n\n5. Optimization Algorithm (Learning Rule)\nThe procedure that adjusts the parameters to minimize the loss.\nExamples: - Gradient Descent and its variants (SGD, Adam) - Expectation-Maximization (EM)\n\n\nGradient descent based on the loss function\nWhen fitting the w, we want to minimize the loss function. Let’s go step by step:\n\n\nGradient of Linear Regression\nWe start with the linear regression model for a single sample (i):\n\\[\n\\hat{y}_i = w x_i + b\n\\]\nFor \\(n\\) samples, the mean squared error (MSE) loss is:\n\\[\nL = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2\n\\]\nSubstituting the model prediction gives us the function we need to minimize:\n\\[\nL = \\frac{1}{n} \\sum_{i=1}^n (w x_i + b - y_i)^2\n\\]\nTo minimize the loss function, we need to calculate the derivatives with respect to \\(w\\) (slope parameter in this case) and b (intercept parameter in this case).\n\nCalculate the gradient (first derivative) with respect to \\(w\\) (slope).\nCalculate the gradient (first derivative) with respect to \\(b\\) (intercept).\n\nTips: Use differential calculus. You can treat the sum like a bracket ( ). Don’t forget the chain rule, if applicable.\n\n\n\n\n\n\nSolution Gradients\n\n\n\n\n\n\n1 Gradient with respect to \\(w\\):\n\\[\n\\frac{\\partial L}{\\partial w}\n= \\frac{1}{n} \\sum_{i=1}^n 2 (w x_i + b - y_i) x_i\n= \\frac{2}{n} \\sum_{i=1}^n (w x_i + b - y_i) x_i\n\\]\n\n\n2 Gradient with respect to \\(b\\):\n\\[\n\\frac{\\partial L}{\\partial b}\n= \\frac{1}{n} \\sum_{i=1}^n 2 (w x_i + b - y_i)\n= \\frac{2}{n} \\sum_{i=1}^n (w x_i + b - y_i)\n\\]\n\n\n\n\n\n\n6 Learning rule\nThe learning rule \\(\\eta\\) (“eta”) helps to regulate updating the all parameters.\n\\[\\theta := \\theta - \\eta \\frac{\\partial L}{\\partial \\theta}\\]\nWe run this updating algorithm in a loop, spanning 1000 iterations (“epochs”). Finally, we have to select a random set of starting parameters.\n\n\nTask: Putting it all together.\nWe are now ready to write the full algorithm.\n\nStart with a new data set, for example age and cortical thickness, generated from this code:\n\n\nimport numpy as np\nn = 100\nage = np.linspace(0, 1, n)  # age, standardized\n\n# Generate target: cortical thickness (in mm)\n# slightly decreasing with age + noise\nthickness = 2.8 - 0.008 * age + np.random.normal(0, 0.05, n)\n\n\nInitialize your weights, w and b, each at starting point 0.0\nFit b and w, with a learning rate of 1e-2 and 1000 iterations using python.\n\n\n\n\n\n\n\nSolution Regression\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Training data\nX = age      # input feature (age, for example)\ny = thickness  # target (e.g., cortical thickness)\nn = len(X)\n\n# Initialize parameters\nw = 0.0\nb = 0.0\neta = 1e-2       # learning rate\nepochs = 2000    # number of iterations\n\n# Gradient descent loop\nfor epoch in range(epochs):\n    y_hat = w * X + b\n    dw = (2/n) * np.sum((y_hat - y) * X)\n    db = (2/n) * np.sum(y_hat - y)\n    \n    # update weights\n    w -= eta * dw\n    b -= eta * db\n    \n    # print progress occasionally\n    if epoch % 200 == 0:\n        loss = np.mean((y_hat - y)**2)\n        print(f\"Epoch {epoch:4d}: w={w:.3f}, b={b:.3f}, Loss={loss:.4f}\")\n\n# Final weights\nprint(f\"Final weights: w={w:.3f}, b={b:.3f}\")\n\n\nplt.figure(figsize=(7,5))\nplt.scatter(age, thickness, color=\"black\", label=\"Data (observed)\", alpha=0.7)\nplt.plot(age, b + w * age, color=\"red\", linewidth=2, label=\"Fitted regression line\")\nplt.xlabel(\"Age (years)\")\nplt.ylabel(\"Cortical thickness (mm)\")\nplt.title(\"Linear Regression Fit via Gradient Descent\")\nplt.legend()\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.savefig(\"imgs/linear_regression_fit.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\nEpoch    0: w=0.028, b=0.056, Loss=7.8276\nEpoch  200: w=0.883, b=2.308, Loss=0.0707\nEpoch  400: w=0.681, b=2.433, Loss=0.0421\nEpoch  600: w=0.521, b=2.518, Loss=0.0255\nEpoch  800: w=0.399, b=2.584, Loss=0.0158\nEpoch 1000: w=0.305, b=2.634, Loss=0.0101\nEpoch 1200: w=0.234, b=2.672, Loss=0.0068\nEpoch 1400: w=0.179, b=2.701, Loss=0.0049\nEpoch 1600: w=0.138, b=2.724, Loss=0.0038\nEpoch 1800: w=0.106, b=2.741, Loss=0.0031\nFinal weights: w=0.081, b=2.754",
    "crumbs": [
      "What is machine learning?",
      "Regression from scratch"
    ]
  },
  {
    "objectID": "ml-overview.html",
    "href": "ml-overview.html",
    "title": "What is machine learning?",
    "section": "",
    "text": "Machine learning and classical statistics look at different questions on different levels (group, individual) and use different methods.",
    "crumbs": [
      "What is machine learning?",
      "What is machine learning?"
    ]
  },
  {
    "objectID": "ml-overview.html#different-types-of-machine-learning",
    "href": "ml-overview.html#different-types-of-machine-learning",
    "title": "What is machine learning?",
    "section": "Different types of machine learning",
    "text": "Different types of machine learning\n\n\n\n\n\n\n\nSupervised Machine Learning\nUnsupervised Machine Learning\n\n\n\n\nInput: X, Y  Output: Patterns or associations between X and Y  Examples:  - Regression of body weight (Y) onto age (X)  - Classification of bird species (Y) based on body weight (X)\nInput: Y  Output: Patterns derived from Y following a set of rules  Example:  - Subgroups within bird species based on characteristics of the beak (Y)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nRule of thumb: start simple (linear/logistic, trees), validate rigorously, then consider more complex models if they add clear value.",
    "crumbs": [
      "What is machine learning?",
      "What is machine learning?"
    ]
  },
  {
    "objectID": "ml-overview.html#concepts-in-machine-learning",
    "href": "ml-overview.html#concepts-in-machine-learning",
    "title": "What is machine learning?",
    "section": "Concepts in Machine learning",
    "text": "Concepts in Machine learning\n\nCross-validation\n\nCross-validation is a technique used to evaluate how well a machine learning model generalizes to new, unseen data.\nThe data is split into several parts (called folds). The model is trained on some folds and tested on the remaining one — this process is repeated so that every fold serves once as a test set.\nThe average performance across all folds gives a more reliable estimate of the model’s accuracy or error than a single train–test split.\nIn short: train multiple times, test multiple times, average the results → estimate generalization.\n\n\n\n\nGrid search\n\nGrid search is a method used to find the best set of hyperparameters for a machine learning model.\nIt systematically tries all possible combinations of parameter values from a predefined “grid,” trains a model for each combination (often using cross-validation), and compares their performance.\nThe combination that gives the best validation score is selected as the optimal set of parameters.\nIn short: try many parameter combinations → evaluate → keep the best one.\n\n\n\nLoss function\n\nA loss function measures how far a model’s predictions are from the true values — it’s what the model tries to minimize during training.\nWhile grid search optimizes hyperparameters (external settings like learning rate or regularization strength), the loss function guides the optimization of the model parameters themselves (e.g., weights and intercepts).\nIn short: the loss function teaches the model, whereas grid search tunes how the model learns.",
    "crumbs": [
      "What is machine learning?",
      "What is machine learning?"
    ]
  },
  {
    "objectID": "ml-overview.html#optimizer",
    "href": "ml-overview.html#optimizer",
    "title": "What is machine learning?",
    "section": "Optimizer",
    "text": "Optimizer\n\nAn optimizer is the algorithm that automatically adjusts the model’s parameters (like weights and intercepts) to minimize the loss function.\nIt’s how the model learns from data.\nAt every step, the optimizer looks at how changing each parameter would affect the loss, and then moves them in the direction that reduces the loss the most.\n\nExamples of optimizers\n\nGradient Descent — the basic algorithm that updates parameters step by step in the direction of the steepest decrease of loss.\nStochastic Gradient Descent (SGD) — updates using small random batches of data (faster for big datasets).\nAdam, RMSProp, Adagrad — advanced optimizers that adapt learning rates automatically for each parameter.",
    "crumbs": [
      "What is machine learning?",
      "What is machine learning?"
    ]
  }
]