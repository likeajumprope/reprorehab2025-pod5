[
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Using Python Virtual Environments (venv) in VS Code",
    "section": "",
    "text": "A virtual environment (venv) isolates a project’s Python packages so different projects can use different versions without conflicts. This guide shows how\n\nto create a venv,\ninstall packages,\nand configure VS Code to use it on macOS/Linux and Windows.",
    "crumbs": [
      "Set-up",
      "Using Python Virtual Environments (venv) in VS Code"
    ]
  },
  {
    "objectID": "setup.html#overview",
    "href": "setup.html#overview",
    "title": "Using Python Virtual Environments (venv) in VS Code",
    "section": "",
    "text": "A virtual environment (venv) isolates a project’s Python packages so different projects can use different versions without conflicts. This guide shows how\n\nto create a venv,\ninstall packages,\nand configure VS Code to use it on macOS/Linux and Windows.",
    "crumbs": [
      "Set-up",
      "Using Python Virtual Environments (venv) in VS Code"
    ]
  },
  {
    "objectID": "setup.html#why-environments-matter-in-python",
    "href": "setup.html#why-environments-matter-in-python",
    "title": "Using Python Virtual Environments (venv) in VS Code",
    "section": "Why environments matter in Python",
    "text": "Why environments matter in Python\n\n\n\n\n\n\nTip\n\n\n\nEnvironments keep your work reproducible, conflict-free, and safe.\n\n\nPython installs packages globally by default. Over time, this can create dependency conflicts—for example:\n\nProject A needs NumPy 1.22, while Project B needs NumPy 2.0.\n\nInstalling new libraries for one project breaks another.\n\nReproducing your setup on another computer (or on a cluster) becomes difficult.\n\nA virtual environment solves this by giving each project its own isolated package space:\n\nYou can install, upgrade, or remove packages without affecting other projects.\n\nYour requirements.txt or environment.yml fully defines what’s needed.\n\nColleagues (or future you) can recreate the same environment anywhere.\n\nThink of it as a sandboxed workspace for each project.\n\n\n\n\n\n\nTip\n\n\n\nWhen to use venv vs conda?\nIf you don’t need compiled scientific stacks managed by conda, venv + pip is lightweight and portable. You can always switch to conda later if needed.",
    "crumbs": [
      "Set-up",
      "Using Python Virtual Environments (venv) in VS Code"
    ]
  },
  {
    "objectID": "setup.html#prerequisites",
    "href": "setup.html#prerequisites",
    "title": "Using Python Virtual Environments (venv) in VS Code",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nPython 3.10+ (latest version is 3.13) installed and available on your PATH (python --version).\nVS Code with the Python and Jupyter extensions.\nBasic terminal (macOS/Linux) or PowerShell (Windows) access.",
    "crumbs": [
      "Set-up",
      "Using Python Virtual Environments (venv) in VS Code"
    ]
  },
  {
    "objectID": "setup.html#create-a-virtual-environment",
    "href": "setup.html#create-a-virtual-environment",
    "title": "Using Python Virtual Environments (venv) in VS Code",
    "section": "1. Create a virtual environment",
    "text": "1. Create a virtual environment\n\nmacOS / Linux\n# in your project folder\npython -m venv .venv\n\n# activate it\nsource .venv/bin/activate\n# your prompt should show (.venv)\n\n\nWindows powershell:\n# in your project folder\npython -m venv .venv\n\n# activate it\n.\\.venv\\Scripts\\Activate.ps1\n# your prompt should show (.venv)\nYou should be able to see a folder .venv in your project folder.\n\n\n\n\n\n\nNote\n\n\n\nThe folder name .venv is conventional and recognized by VS Code automatically if it lives in the project root.",
    "crumbs": [
      "Set-up",
      "Using Python Virtual Environments (venv) in VS Code"
    ]
  },
  {
    "objectID": "setup.html#install-your-project-packages",
    "href": "setup.html#install-your-project-packages",
    "title": "Using Python Virtual Environments (venv) in VS Code",
    "section": "2. Install your project packages",
    "text": "2. Install your project packages\nWith the environment activated, install what you need in the terminal. The main installer in python is pip, while other, newer (better) options do exist (uv, conda).\npip install numpy pandas matplotlib scikit-learn jupyter",
    "crumbs": [
      "Set-up",
      "Using Python Virtual Environments (venv) in VS Code"
    ]
  },
  {
    "objectID": "setup.html#open-the-project-in-vs-code",
    "href": "setup.html#open-the-project-in-vs-code",
    "title": "Using Python Virtual Environments (venv) in VS Code",
    "section": "3. Open the project in VS Code",
    "text": "3. Open the project in VS Code\n\nFile → Open Folder… and select your project folder (the one containing .venv).\nVS Code usually detects the environment. If not, select it manually (next section).",
    "crumbs": [
      "Set-up",
      "Using Python Virtual Environments (venv) in VS Code"
    ]
  },
  {
    "objectID": "setup.html#select-the-interpreter-in-vs-code",
    "href": "setup.html#select-the-interpreter-in-vs-code",
    "title": "Using Python Virtual Environments (venv) in VS Code",
    "section": "4. Select the interpreter in VS Code",
    "text": "4. Select the interpreter in VS Code\n1 Press ⌘⇧P / Ctrl+Shift+P → Python: Select Interpreter\n2.  Choose your environment path:\n./.venv/bin/python           # macOS/Linux\n.\\.venv\\Scripts\\python.exe   # Windows",
    "crumbs": [
      "Set-up",
      "Using Python Virtual Environments (venv) in VS Code"
    ]
  },
  {
    "objectID": "setup.html#jupyter-notebooks-optional",
    "href": "setup.html#jupyter-notebooks-optional",
    "title": "Using Python Virtual Environments (venv) in VS Code",
    "section": "5. Jupyter & notebooks (optional)",
    "text": "5. Jupyter & notebooks (optional)\nWhen you create or open a notebook (.ipynb), click the kernel picker (top-right) and select your .venv interpreter. Select your .venv interpreter",
    "crumbs": [
      "Set-up",
      "Using Python Virtual Environments (venv) in VS Code"
    ]
  },
  {
    "objectID": "setup.html#test-your-environment.",
    "href": "setup.html#test-your-environment.",
    "title": "Using Python Virtual Environments (venv) in VS Code",
    "section": "6 Test your environment.",
    "text": "6 Test your environment.\n\nActivate your environment in your terminal. Install the numpy package\n\npip install numpy\n\nCreate a file test.py. Import the numpy package\n\nimport numpy as np",
    "crumbs": [
      "Set-up",
      "Using Python Virtual Environments (venv) in VS Code"
    ]
  },
  {
    "objectID": "More_clustering_algorithms.html",
    "href": "More_clustering_algorithms.html",
    "title": "More Sophisticated Clustering Algorithms",
    "section": "",
    "text": "import time\nimport warnings\nfrom itertools import cycle, islice\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\nLet’s generate some data:\n\n# ============\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n# ============\nn_samples = 500\nseed = 30\nnoisy_circles = datasets.make_circles(\n    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed\n)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)\nrng = np.random.RandomState(seed)\nno_structure = rng.rand(n_samples, 2), None\n\n# Anisotropicly distributed data\nrandom_state = 170\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)\n\n\n# ============\n# Set up cluster parameters\n# ============\nplt.figure(figsize=(9 * 2 + 3, 13))\nplt.subplots_adjust(\n    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\n    \"quantile\": 0.3,\n    \"eps\": 0.3,\n    \"damping\": 0.9,\n    \"preference\": -200,\n    \"n_neighbors\": 3,\n    \"n_clusters\": 3,\n    \"min_samples\": 7,\n    \"xi\": 0.05,\n    \"min_cluster_size\": 0.1,\n    \"allow_single_cluster\": True,\n    \"hdbscan_min_cluster_size\": 15,\n    \"hdbscan_min_samples\": 3,\n    \"random_state\": 42,\n}\n\ndatasets = [\n    (\n        noisy_circles,\n        {\n            \"damping\": 0.77,\n            \"preference\": -240,\n            \"quantile\": 0.2,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.08,\n        },\n    ),\n    (\n        noisy_moons,\n        {\n            \"damping\": 0.75,\n            \"preference\": -220,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n        },\n    ),\n    (\n        varied,\n        {\n            \"eps\": 0.18,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.01,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (\n        aniso,\n        {\n            \"eps\": 0.15,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (blobs, {\"min_samples\": 7, \"xi\": 0.1, \"min_cluster_size\": 0.2}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = StandardScaler().fit_transform(X)\n\n    # estimate bandwidth for mean shift\n    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n\n    # connectivity matrix for structured Ward\n    connectivity = kneighbors_graph(\n        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n    )\n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    two_means = cluster.MiniBatchKMeans(\n        n_clusters=params[\"n_clusters\"],\n        random_state=params[\"random_state\"],\n    )\n    ward = cluster.AgglomerativeClustering(\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n    )\n    spectral = cluster.SpectralClustering(\n        n_clusters=params[\"n_clusters\"],\n        eigen_solver=\"arpack\",\n        affinity=\"nearest_neighbors\",\n        random_state=params[\"random_state\"],\n    )\n    dbscan = cluster.DBSCAN(eps=params[\"eps\"])\n    #hdbscan = cluster.HDBSCAN(\n    #    min_samples=params[\"hdbscan_min_samples\"],\n    #    min_cluster_size=params[\"hdbscan_min_cluster_size\"],\n    #    allow_single_cluster=params[\"allow_single_cluster\"],\n    #)\n    optics = cluster.OPTICS(\n        min_samples=params[\"min_samples\"],\n        xi=params[\"xi\"],\n        min_cluster_size=params[\"min_cluster_size\"],\n    )\n    affinity_propagation = cluster.AffinityPropagation(\n        damping=params[\"damping\"],\n        preference=params[\"preference\"],\n        random_state=params[\"random_state\"],\n    )\n    average_linkage = cluster.AgglomerativeClustering(\n        linkage=\"average\",\n        metric=\"cityblock\",\n        n_clusters=params[\"n_clusters\"],\n        connectivity=connectivity,\n    )\n    birch = cluster.Birch(n_clusters=params[\"n_clusters\"])\n    gmm = mixture.GaussianMixture(\n        n_components=params[\"n_clusters\"],\n        covariance_type=\"full\",\n        random_state=params[\"random_state\"],\n    )\n\n    clustering_algorithms = (\n        (\"MiniBatch\\nKMeans\", two_means),\n        (\"Affinity\\nPropagation\", affinity_propagation),\n        (\"MeanShift\", ms),\n        (\"Spectral\\nClustering\", spectral),\n        (\"Ward\", ward),\n        (\"Agglomerative\\nClustering\", average_linkage),\n        (\"DBSCAN\", dbscan),\n        #(\"HDBSCAN\", hdbscan),\n        (\"OPTICS\", optics),\n        (\"BIRCH\", birch),\n        (\"Gaussian\\nMixture\", gmm),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = time.time()\n\n        # catch warnings related to kneighbors_graph\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \" &gt; 1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Graph is not fully connected, spectral embedding\"\n                + \" may not work as expected.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = time.time()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=18)\n\n        colors = np.array(\n            list(\n                islice(\n                    cycle(\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        # add black color for outliers (if any)\n        colors = np.append(colors, [\"#000000\"])\n        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        plt.xlim(-2.5, 2.5)\n        plt.ylim(-2.5, 2.5)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=plt.gca().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\nplt.show()\n\n/Users/johbay/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1930: FutureWarning:\n\nThe default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/johbay/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1930: FutureWarning:\n\nThe default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/johbay/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1930: FutureWarning:\n\nThe default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/johbay/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1930: FutureWarning:\n\nThe default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/johbay/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1930: FutureWarning:\n\nThe default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/johbay/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1930: FutureWarning:\n\nThe default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\n\n\n\n\n\n\n\nMore sophisticated models using sci-kit learn\nSklearn provides access to a variety of clustering algorithms\n\nMiniBatch K-Means\n\n\nHow it Works: MiniBatch K-Means is a variant of K-Means that uses small random batches of the data to perform clustering updates, making it faster and more memory-efficient than regular K-Means.\nMechanism: The algorithm randomly samples small batches of data points in each iteration, computes cluster assignments, and updates centroids based on the sample.\nUse Case: Best for clustering large datasets where traditional K-Means would be too slow or memory-intensive.\n\n\nAffinity Propagation\n\n\nHow it Works: Affinity Propagation doesn’t require a predefined number of clusters. Instead, it uses a similarity matrix to identify exemplars (representative points for each cluster).\nMechanism: Points communicate through “messages” in a matrix until they converge on exemplars, using parameters like damping and preference to adjust message influence and cluster count.\nUse Case: Suitable for data with well-defined clusters where the number of clusters isn’t known beforehand, but computationally intensive on large datasets.\n\n\nMeanShift\n\n\nHow it Works: MeanShift clusters points by iteratively moving towards the densest regions, identifying clusters as peaks in data density.\nMechanism: The bandwidth parameter controls the neighborhood size. Points are pulled towards their local density center, and clusters form around these peaks.\nUse Case: Good for finding arbitrarily shaped clusters without needing to set a fixed number of clusters, but slow with high-dimensional data.\n\n\nSpectral Clustering\n\n\nHow it Works: Spectral Clustering applies eigenvalue decomposition to a similarity matrix to reduce dimensions and enhance separability before applying clustering.\nMechanism: Constructs a similarity (affinity) matrix of the data, reduces dimensions with eigenvectors, and clusters the data based on these new dimensions.\nUse Case: Suitable for data with complex, non-convex shapes but computationally expensive on large datasets due to matrix decomposition.\n\n\nWard’s Agglomerative Clustering\n\n\nHow it Works: Ward’s method is a hierarchical clustering approach that starts with each data point as its own cluster, then merges pairs of clusters to minimize variance.\nMechanism: The algorithm iteratively merges clusters to minimize the within-cluster variance, based on linkage (distance between clusters).\nUse Case: Works well with small to medium-sized datasets where clusters have a clear hierarchy; not ideal for very large datasets due to high computational complexity.\n\n\nAgglomerative Clustering (Average Linkage)\n\n\nHow it Works: Another hierarchical approach, it merges clusters based on average linkage, which minimizes the average pairwise distance between clusters.\nMechanism: Begins with each data point in its own cluster, then iteratively merges clusters based on the average distance between all pairs of points in different clusters.\nUse Case: Useful for hierarchical clustering when clusters are of varying sizes or non-convex shapes.\n\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n\n\nHow it Works: DBSCAN forms clusters based on the density of points, where points in dense regions are considered part of the same cluster, and outliers are marked separately.\nMechanism: Uses two parameters, eps (maximum distance between points) and min_samples (minimum number of points in a neighborhood), to form clusters based on point density.\nUse Case: Effective for identifying clusters of varying shapes and sizes with noise/outliers, especially in spatial data.\n\n\nOPTICS (Ordering Points To Identify the Clustering Structure)\n\n\nHow it Works: OPTICS is similar to DBSCAN but more flexible in finding clusters with varying densities by generating an ordering of points and cluster reachability distances.\nMechanism: Unlike DBSCAN, it doesn’t produce explicit clusters initially but instead creates an ordered list of points with reachability distances that can later be clustered based on density.\nUse Case: Useful for data with clusters of varying density, as it can adaptively adjust for different densities and still handle noise well.\n\n\nBIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)\n\n\nHow it Works: BIRCH builds a tree structure of data points, clustering as it builds, making it efficient for large datasets.\nMechanism: Uses a tree structure called a Clustering Feature Tree to incrementally cluster incoming points based on a predefined threshold for cluster compactness.\nUse Case: Ideal for large datasets where a single pass through the data is beneficial, but may struggle with clusters of varying densities.\n\n\nGaussian Mixture Model (GMM)\n\n\nHow it Works: GMM assumes that the data is generated from a mixture of Gaussian distributions and uses Expectation-Maximization (EM) to estimate the parameters.\nMechanism: GMM represents clusters as Gaussian distributions and iteratively refines cluster assignments by maximizing likelihood, allowing for flexible cluster shapes.\nUse Case: Best for data that roughly follows a Gaussian distribution; works well on data where clusters may overlap but can be slow on large datasets.\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nMain Idea\nShape of Clusters\nOptimal Use Case\n\n\n\n\nMiniBatch K-Means\nFast K-means for large datasets\nSpherical\nLarge datasets needing fast clustering\n\n\nAffinity Propagation\nMessage-passing clustering\nArbitrary\nUnknown cluster count, smaller datasets\n\n\nMeanShift\nDensity peak clustering\nArbitrary\nNon-parametric clustering for complex shapes\n\n\nSpectral Clustering\nEigen-decomposition + clustering\nNon-convex\nComplex, non-convex clusters\n\n\nWard’s Agglomerative\nHierarchical clustering by variance\nHierarchical, generally convex\nSmall, hierarchical data\n\n\nAverage Linkage\nHierarchical by average linkage\nHierarchical\nVaried shapes with hierarchical clusters\n\n\nDBSCAN\nDensity-based clustering\nArbitrary\nClusters with noise, varying shapes\n\n\nOPTICS\nDensity-based, adaptive to density\nArbitrary\nClusters with varying densities\n\n\nBIRCH\nTree-based clustering for large data\nGenerally convex\nLarge datasets needing efficient clustering\n\n\nGaussian Mixture\nProbabilistic, Gaussian distribution\nElliptical or spherical\nOverlapping clusters with Gaussian assumptions",
    "crumbs": [
      "What is machine learning?",
      "More Sophisticated Clustering Algorithms"
    ]
  },
  {
    "objectID": "Dimensionality_reduction.html",
    "href": "Dimensionality_reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "Dimensionality reduction is the art of compressing data without losing its essence.\nWhy do we need dimensionality reduction?\nExample:\nCoding example:\nLet’s generate some simple data set.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data                  # shape (150, 4) = sepal len/width, petal len/width\ny = iris.target                # 0,1,2\ntarget_names = iris.target_names",
    "crumbs": [
      "What is machine learning?",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "Dimensionality_reduction.html#standardize-the-data",
    "href": "Dimensionality_reduction.html#standardize-the-data",
    "title": "Dimensionality Reduction",
    "section": "Standardize the data",
    "text": "Standardize the data\nStandardizing is important as otherwise one “longer” dimension overpowers the “shorter” ones.\n\nmu = X.mean(axis=0)                  # (d,)\nsigma = X.std(axis=0, ddof=1)        # (d,)\nX_std = (X - mu) / sigma",
    "crumbs": [
      "What is machine learning?",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "Dimensionality_reduction.html#calculate-the-covariance-matrix",
    "href": "Dimensionality_reduction.html#calculate-the-covariance-matrix",
    "title": "Dimensionality Reduction",
    "section": "Calculate the covariance matrix",
    "text": "Calculate the covariance matrix\nNext we calculate the covariance matrix for X_std.\nThe covariance is given by:\n\\[\\text{cov}(X, Y) = \\frac{1}{n - 1} \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})\\]\nYou can use the fact that the unnormalized covariance of a matrix X is given by:\n\\[\n{X}^T {X} =\n\\begin{bmatrix}\n\\sum \\tilde{x}_1^2 & \\sum \\tilde{x}_1 \\tilde{x}_2 & \\cdots \\\\\n\\sum \\tilde{x}_2 \\tilde{x}_1 & \\sum \\tilde{x}_2^2 & \\cdots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{bmatrix}\n\\]\n\nn_samples = X_std.shape[0]\nCov = (X_std.T @ X_std) / (n_samples - 1)",
    "crumbs": [
      "What is machine learning?",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "Dimensionality_reduction.html#getting-the-eigen-values",
    "href": "Dimensionality_reduction.html#getting-the-eigen-values",
    "title": "Dimensionality Reduction",
    "section": "Getting the eigen values",
    "text": "Getting the eigen values\nThe formal definition of the eigen values decomposition for a square matrix A is: \\[\n\\begin{aligned}\nA \\cdot v &= \\lambda v \\\\\nA \\cdot v - \\lambda v &= 0 \\\\\n(A - \\lambda I) v &= 0\n\\end{aligned}\n\\]\nWe now solve for \\(\\lambda\\) in the determinant of this equation:\n\\[det(A \\cdot v - \\lambda v )= 0\\] which gives a polynomoal in \\(\\lambda\\). Solve for \\(\\lambda\\) (the roots are the eigen values).\nSuppose a transformation (matrix) stretches space differently along different axes: \\[\nA = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{bmatrix}\n\\]\n\\[\\begin{aligned}\ndet(A \\cdot v - \\lambda v )= 0 \\\\\n\\end{aligned}\\]\nbecomes:\n\\[\n\\det(A - \\lambda I) =\n\\begin{vmatrix}\n2 - \\lambda & 1 \\\\\n1 & 2 - \\lambda\n\\end{vmatrix}\n= (2 - \\lambda)(2 - \\lambda) - (1)(1)\n= (2 - \\lambda)^2 - 1\n= \\lambda^2 - 4 \\lambda + 3\n\\]\nThis leads us to the solutions \\(\\lambda\\) = 1 and \\(lambda\\) = 3.",
    "crumbs": [
      "What is machine learning?",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "Dimensionality_reduction.html#getting-the-eigenvectors",
    "href": "Dimensionality_reduction.html#getting-the-eigenvectors",
    "title": "Dimensionality Reduction",
    "section": "Getting the eigenvectors",
    "text": "Getting the eigenvectors\nTo get the eigenvectors, we have to plug in the eigenvalues into the equation above and solve for \\(v\\): \\[\n(A - \\lambda I) v = 0\\]\nFor λ = 3 \\[\nA - 3I\n= \\begin{pmatrix}2 & 1\\\\[2pt] 1 & 2\\end{pmatrix}\n- 3\\begin{pmatrix}1 & 0\\\\[2pt] 0 & 1\\end{pmatrix}\n= \\begin{pmatrix}2-3 & 1\\\\[2pt] 1 & 2-3\\end{pmatrix}\n= \\begin{pmatrix}-1 & 1\\\\[2pt] 1 & -1\\end{pmatrix}.\n\\]\nFrom this,set up the equation: \\[\n\\begin{pmatrix}\n-1 & 1 \\\\\n1 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\\ny\n\\end{pmatrix}\n= \\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\n\\]\nThis gives the equation: \\[\nx + y = 0\n\\quad \\Rightarrow \\quad y = x\n\\] So, any vector of the form\n\\[\\begin{pmatrix} x \\\\ x \\end{pmatrix}\\]\nis an eigenvector. Choose a simple one like x = 1: \\[v_1 =\n\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}\n\\]\nFor λ = 1\n\\[\nA - I\n= \\begin{pmatrix}2 & 1\\\\[2pt] 1 & 2\\end{pmatrix}\n- \\begin{pmatrix}1 & 0\\\\[2pt] 0 & 1\\end{pmatrix}\n= \\begin{pmatrix}1 & 1\\\\[2pt] 1 & 1\\end{pmatrix}.\n\\]\nSet up the equation: \\[\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\\ny\n\\end{pmatrix}\n= \\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\n\\]\nThis gives the equation: \\[\nx + y = 0\n\\quad \\Rightarrow \\quad y = -x\n\\] So any vector of the form\n\\[\\begin{pmatrix} x \\\\ -x \\end{pmatrix}\\]\nis an eigenvector. Choose x = 1:\n\\[\nv_2 =\n\\begin{pmatrix}\n1 \\\\\n-1\n\\end{pmatrix}\n\\]",
    "crumbs": [
      "What is machine learning?",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "Dimensionality_reduction.html#final-eigenpairs",
    "href": "Dimensionality_reduction.html#final-eigenpairs",
    "title": "Dimensionality Reduction",
    "section": "Final Eigenpairs",
    "text": "Final Eigenpairs\n\\[\n\\lambda = 3,\n\\] \\[\nv_1 = \\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}\n\\]\n\\[\n\\lambda_2 = 1,\n\\] \\[\nv_2 = \\begin{pmatrix}\n1 \\\\\n-1\n\\end{pmatrix}\n\\]",
    "crumbs": [
      "What is machine learning?",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "Dimensionality_reduction.html#in-python",
    "href": "Dimensionality_reduction.html#in-python",
    "title": "Dimensionality Reduction",
    "section": "In python:",
    "text": "In python:\n\neigvals, eigvecs = np.linalg.eigh(Cov)     # eigvecs columns = eigenvectors\n# sort by descending eigenvalue (most variance first)\nidx = np.argsort(eigvals)[::-1]\neigvals = eigvals[idx]\neigvecs = eigvecs[:, idx]\nprint(eigvals)\nprint(eigvecs)\n\n[2.91849782 0.91403047 0.14675688 0.02071484]\n[[-0.52106591  0.37741762  0.71956635  0.26128628]\n [ 0.26934744  0.92329566 -0.24438178 -0.12350962]\n [-0.5804131   0.02449161 -0.14212637 -0.80144925]\n [-0.56485654  0.06694199 -0.63427274  0.52359713]]\n\n\n\nimport pandas as pd\nloadings = pd.DataFrame(\n    eigvecs,\n    index=feature_names,\n    columns=[f\"PC{i}\" for i in range(1, eigvecs.shape[1]+1)]\n)\n\nprint(\"\\n=== PCA Eigenvalues & Explained Variance ===\")\nprint(loadings.round(4))\n\n\n=== PCA Eigenvalues & Explained Variance ===\n                      PC1     PC2     PC3     PC4\nsepal length (cm) -0.5211  0.3774  0.7196  0.2613\nsepal width (cm)   0.2693  0.9233 -0.2444 -0.1235\npetal length (cm) -0.5804  0.0245 -0.1421 -0.8014\npetal width (cm)  -0.5649  0.0669 -0.6343  0.5236",
    "crumbs": [
      "What is machine learning?",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "Dimensionality_reduction.html#plotting",
    "href": "Dimensionality_reduction.html#plotting",
    "title": "Dimensionality Reduction",
    "section": "Plotting",
    "text": "Plotting\n\nZ = X_std @ eigvecs[:, :2]  # shape: (n_samples, 2)\nprint(Z.shape)\n\n(150, 2)\n\n\n\nplt.figure(figsize=(6,5))\nfor label in np.unique(y):\n    mask = (y == label)\n    plt.scatter(Z[mask, 0], Z[mask, 1],\n                alpha=0.7, s=40, label=target_names[label])\n\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.title(\"Iris projected onto first two principal components\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# --- P Plot eigenvectors as red arrows\n# Scale arrows by eigenvalues (optional: to make them more visible)\nscaling_factor = 3  # tweak for visibility\nfor i in range(4):\n    plt.arrow(0, 0, eigvecs[i, 0]*scaling_factor, eigvecs[i, 1]*scaling_factor,\n              color='red', width=0.015, head_width=0.15)\n    plt.text(eigvecs[i, 0]*scaling_factor*1.15,\n             eigvecs[i, 1]*scaling_factor*1.15,\n             feature_names[i],\n             color='red', fontsize=10)\nfor label in np.unique(y):\n    mask = (y == label)\n    plt.scatter(Z[mask, 0], Z[mask, 1],\n                alpha=0.7, s=40, label=target_names[label])\n\n# ---  Label axes with variance explained\nexplained_var_ratio = eigvals / eigvals.sum()\nplt.xlabel(f\"PC1 ({explained_var_ratio[0]*100:.1f}% variance)\")\nplt.ylabel(f\"PC2 ({explained_var_ratio[1]*100:.1f}% variance)\")\n\nplt.title(\"PCA Biplot — Data and Loadings (Iris dataset)\")\nplt.legend()\nplt.axhline(0, color='gray', linewidth=0.5)\nplt.axvline(0, color='gray', linewidth=0.5)\nplt.grid(True, linestyle='--', alpha=0.4)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "What is machine learning?",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "Dimensionality_reduction.html#putting-it-all-together",
    "href": "Dimensionality_reduction.html#putting-it-all-together",
    "title": "Dimensionality Reduction",
    "section": "Putting it all together:",
    "text": "Putting it all together:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# 1) Load data\nX, y = load_iris(return_X_y=True)\ntarget_names = [\"setosa\", \"versicolor\", \"virginica\"]\n\n# 2) Standardize the data (important for PCA)\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X)\n\n# 3) Apply PCA to reduce to 2 components (for 2D plotting)\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_std)\n\n# 4) Plot PC1 vs PC2 (color by species)\nplt.figure(figsize=(6,5))\nfor label in np.unique(y):\n    mask = (y == label)\n    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], s=30, alpha=0.85, label=target_names[label])\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(\"Iris: PCA to 2D\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# 5) Explained variance ratio (how much each PC explains)\nprint(\"Explained variance ratio:\", np.round(pca.explained_variance_ratio_, 4))\nprint(\"Cumulative explained variance:\", np.round(np.cumsum(pca.explained_variance_ratio_), 4))\n\n# 6) Scree plot for all components (fit full PCA)\npca_full = PCA(n_components=None)\npca_full.fit(X_std)\nevr = pca_full.explained_variance_ratio_\ncum = np.cumsum(evr)\n\nplt.figure(figsize=(6,4))\npcs = np.arange(1, len(evr)+1)\nplt.bar(pcs, evr, alpha=0.75, label=\"EVR per PC\")\nplt.plot(pcs, cum, marker=\"o\", label=\"Cumulative EVR\")\nplt.axhline(0.95, linestyle=\"--\", label=\"95% threshold\")\nplt.xticks(pcs)\nplt.xlabel(\"Principal Component\")\nplt.ylabel(\"Explained Variance Ratio\")\nplt.title(\"PCA Explained Variance (Iris)\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nExplained variance ratio: [0.7296 0.2285]\nCumulative explained variance: [0.7296 0.9581]\n\n\n\n\n\n\n\n\n\n\nTask: PCA using the wine data set.\nRepeat the analysis (either scikit learn or the original one) using the wine data set\nThe wine data set:\n•   178 samples × 13 features\n•   3 wine classes (like 3D clusters in higher space)\nCode:\n\nfrom sklearn.datasets import load_wine\nwine = load_wine()\nX = wine.data\ny = wine.target\nfeature_names = wine.feature_names\ntarget_names = wine.target_names\n\nSteps:\n\nVisulalize the data set (some of it)\nStandardize the data set\nGet the covariance matrix (pro: visulalize using a heatmpa)\nCalculate eigenvalues and eigenvectors\nSelect first two principal components\nTransform and plot data into principal components\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# ----------------------------\n# 1) Load & visualize \"some of it\"\n# ----------------------------\nwine = load_wine()\nX = wine.data                  # (178, 13)\ny = wine.target                # 3 classes\nfeature_names = wine.feature_names\ntarget_names = wine.target_names\n\nprint(\"X shape:\", X.shape)\nprint(\"First 5 rows:\\n\", np.round(X[:5], 2))\n\n# quick scatter of two raw features (alcohol vs. malic_acid) just to \"see\" the data\nplt.figure(figsize=(6,5))\nfor lab in np.unique(y):\n    m = (y == lab)\n    plt.scatter(X[m, 0], X[m, 1], s=25, alpha=0.7, label=target_names[lab])\nplt.xlabel(feature_names[0]); plt.ylabel(feature_names[1])\nplt.title(\"Wine (raw): alcohol vs malic_acid\")\nplt.legend(); plt.tight_layout(); plt.show()\n\n# ----------------------------\n# 2) Standardize (mean 0, std 1 per feature)\n# ----------------------------\nmu = X.mean(axis=0)\nsigma = X.std(axis=0, ddof=1)\nX_std = (X - mu) / sigma\n\n# ----------------------------\n# 3) Covariance matrix + heatmap\n#    Cov = (1/(n-1)) * X_std^T X_std\n# ----------------------------\nn = X_std.shape[0]\nCov = (X_std.T @ X_std) / (n - 1)    # (13 x 13)\n\nplt.figure(figsize=(7.5,6))\nim = plt.imshow(Cov, cmap=\"viridis\")\nplt.colorbar(im, fraction=0.046, pad=0.04)\nplt.xticks(range(len(feature_names)), feature_names, rotation=60, ha='right', fontsize=9)\nplt.yticks(range(len(feature_names)), feature_names, fontsize=9)\nplt.title(\"Wine: covariance matrix (standardized features)\")\nplt.tight_layout(); plt.show()\n\n# ----------------------------\n# 4) Eigenvalues & eigenvectors (sorted desc)\n# ----------------------------\neigvals, eigvecs = np.linalg.eigh(Cov)         # symmetric -&gt; eigh\nidx = np.argsort(eigvals)[::-1]\neigvals = eigvals[idx]\neigvecs = eigvecs[:, idx]                      # columns are principal directions\n\nprint(\"Top 5 eigenvalues:\", np.round(eigvals[:5], 4))\nevr = eigvals / eigvals.sum()                  # explained variance ratio\nprint(\"Explained variance ratio (first 5):\", np.round(evr[:5], 4))\nprint(\"Cumulative EVR (first 5):\", np.round(np.cumsum(evr[:5]), 4))\n\n# Scree plot\nplt.figure(figsize=(6,4))\npcs = np.arange(1, len(eigvals)+1)\nplt.bar(pcs, evr, alpha=0.75, label=\"EVR per PC\")\nplt.plot(pcs, np.cumsum(evr), marker=\"o\", label=\"Cumulative EVR\")\nplt.axhline(0.95, ls=\"--\", label=\"95%\")\nplt.xticks(pcs)\nplt.xlabel(\"Principal Component\"); plt.ylabel(\"Explained Variance Ratio\")\nplt.title(\"PCA Explained Variance (Wine)\"); plt.legend()\nplt.tight_layout(); plt.show()\n\n# ----------------------------\n# 5) Select first two principal components\n# ----------------------------\nW2 = eigvecs[:, :2]            # (13 x 2)\n\n# ----------------------------\n# 6) Transform and plot data in PC1–PC2 space\n# ----------------------------\nZ = X_std @ W2                 # (178 x 2)\n\nplt.figure(figsize=(6,5))\nfor lab in np.unique(y):\n    m = (y == lab)\n    plt.scatter(Z[m, 0], Z[m, 1], s=30, alpha=0.8, label=target_names[lab])\nplt.xlabel(f\"PC1 ({evr[0]*100:.1f}% var)\")\nplt.ylabel(f\"PC2 ({evr[1]*100:.1f}% var)\")\nplt.title(\"Wine projected onto first two principal components\")\nplt.legend(); plt.tight_layout(); plt.show()\n\n# (Optional) Biplot-style loadings: show how original features contribute to PC1/PC2\nscale = 3.0\nplt.figure(figsize=(7,6))\nfor lab in np.unique(y):\n    m = (y == lab)\n    plt.scatter(Z[m, 0], Z[m, 1], s=20, alpha=0.6, label=target_names[lab])\n\nfor i, name in enumerate(feature_names[:2 ]):\n    vx, vy = W2[i, 0], W2[i, 1]      # loadings on PC1/PC2 for feature i\n    plt.arrow(0, 0, scale*vx, scale*vy, color='red', width=0.01, head_width=0.12)\n    plt.text(scale*vx*1.08, scale*vy*1.08, name, color='red', fontsize=9)\n\nplt.xlabel(f\"PC1 ({evr[0]*100:.1f}% var)\"); plt.ylabel(f\"PC2 ({evr[1]*100:.1f}% var)\")\nplt.title(\"Wine PCA biplot (data + feature loadings)\")\nplt.axhline(0, color='gray', lw=0.6); plt.axvline(0, color='gray', lw=0.6)\nplt.grid(True, ls='--', alpha=0.35); plt.legend()\nplt.tight_layout(); plt.show()\n\nX shape: (178, 13)\nFirst 5 rows:\n [[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n  2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n [1.320e+01 1.780e+00 2.140e+00 1.120e+01 1.000e+02 2.650e+00 2.760e+00\n  2.600e-01 1.280e+00 4.380e+00 1.050e+00 3.400e+00 1.050e+03]\n [1.316e+01 2.360e+00 2.670e+00 1.860e+01 1.010e+02 2.800e+00 3.240e+00\n  3.000e-01 2.810e+00 5.680e+00 1.030e+00 3.170e+00 1.185e+03]\n [1.437e+01 1.950e+00 2.500e+00 1.680e+01 1.130e+02 3.850e+00 3.490e+00\n  2.400e-01 2.180e+00 7.800e+00 8.600e-01 3.450e+00 1.480e+03]\n [1.324e+01 2.590e+00 2.870e+00 2.100e+01 1.180e+02 2.800e+00 2.690e+00\n  3.900e-01 1.820e+00 4.320e+00 1.040e+00 2.930e+00 7.350e+02]]\nTop 5 eigenvalues: [4.7059 2.497  1.4461 0.919  0.8532]\nExplained variance ratio (first 5): [0.362  0.1921 0.1112 0.0707 0.0656]\nCumulative EVR (first 5): [0.362  0.5541 0.6653 0.736  0.8016]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore stuff\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\n\n\ncomponents = pca.components_   # eigenvectors (directions)\nscores = X_pca                 # principal component scores\nX_centered = X_std             # standardized data is already centered\n\n# 4️⃣ First eigenvector (red arrow)\nv1 = components[0]\norigin = np.mean(X_centered, axis=0)\n\n# 5️⃣ Plot\nfig, ax = plt.subplots(figsize=(7,7))\nax.scatter(X_centered[:,0], X_centered[:,1], alpha=0.3, label='Data (centered)')\n\n# Eigenvector direction\nax.arrow(origin[0], origin[1], v1[0]*2, v1[1]*2, \n         color='red', width=0.2, head_width=0.4, label='1st eigenvector')\n\n# Projection of data onto the 1st eigenvector\nproj = np.outer(scores[:,0], v1)\nax.scatter(proj[:,0], proj[:,1], color='blue', alpha=0.5, label='Projections (1st PC)')\n\nax.set_aspect('equal')\nax.legend()\nax.set_title(\"Eigenvector (direction) vs. Principal Component (projection)\")\nplt.xlabel(\"Feature 1 (standardized)\")\nplt.ylabel(\"Feature 2 (standardized)\")\nplt.show()",
    "crumbs": [
      "What is machine learning?",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "Regression_from_scratch.html",
    "href": "Regression_from_scratch.html",
    "title": "Regression from scratch",
    "section": "",
    "text": "Symbol Type\nExample\nTypical Shape\nDescription\n\n\n\n\nScalar\n\\(a, b, x_i, y, \\eta\\)\n\\(1 \\times 1\\)\nSingle numeric value (e.g., a feature value, bias, or loss).\n\n\nVector\n\\(\\mathbf{x}, \\mathbf{w}, \\boldsymbol{\\theta}\\)\n\\(p \\times 1\\)\nColumn vector containing multiple values (e.g., features or parameters).\n\n\nMatrix\n\\(\\mathbf{X}, \\mathbf{W}, \\mathbf{A}\\)\n\\(n \\times p\\)\n2-D array (rows = samples, columns = features).\n\n\nDot product\n\\(\\mathbf{w}^T \\mathbf{x}\\)\nscalar\nInner product between two vectors.\n\n\nOuter product\n\\(\\mathbf{x}\\mathbf{w}^T\\)\n\\(p \\times p\\)\nCreates a matrix from two vectors.\n\n\nInverse\n\\(\\mathbf{A}^{-1}\\)\n\\(p \\times p\\)\nMatrix that “undoes” multiplication by \\(\\mathbf{A}\\), if it exists.\n\n\nHat notation\n\\(\\hat{y}\\)\nscalar or vector\nEstimated or predicted value (e.g., \\(\\hat{y}\\) = model prediction).\n\n\nBar notation\n\\(\\bar{x}\\)\nscalar or vector\nMean or average value (e.g., \\(\\bar{x}\\) = sample mean).",
    "crumbs": [
      "What is machine learning?",
      "Regression from scratch"
    ]
  },
  {
    "objectID": "Regression_from_scratch.html#the-model",
    "href": "Regression_from_scratch.html#the-model",
    "title": "Regression from scratch",
    "section": "1. The model",
    "text": "1. The model\nThe mathematical structure or function family that maps inputs to outputs (supervised learning). The model defines what forms of relationships between input and output can be learned.\n\nFor linear regression:\n\n\n1. Scalar form\nEach observation (i) has its own equation: \\[\n\\hat{y}_i = w_0 + w_1 x_{i1} + w_2 x_{i2} + \\dots + w_p x_{ip} + \\varepsilon_i\n\\]\n\n\n2. Vector (standard math) form\nCompact form for a single observation using vectors: \\[\n\\hat{y}_i = \\mathbf{w}^T \\mathbf{x}_i + b + \\varepsilon_i\n\\]\n\n\n3. Matrix form\nAll (n) observations combined (X stacks row vectors): \\[\n\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w} + \\mathbf{b} + \\boldsymbol{\\varepsilon}\n\\] or (if (b) is absorbed as an intercept column of ones in ()): \\[\n\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w} + \\boldsymbol{\\varepsilon}\n\\]",
    "crumbs": [
      "What is machine learning?",
      "Regression from scratch"
    ]
  },
  {
    "objectID": "Regression_from_scratch.html#input-data",
    "href": "Regression_from_scratch.html#input-data",
    "title": "Regression from scratch",
    "section": "2. Input Data",
    "text": "2. Input Data\n\nExamples: images, text, sensor data, neuroimaging features, etc.\nOften represented as a matrix X of shape \\((n_{samples}, n_{features})\\).\nData needs to be cleaned, normalized, encoded, and sometimes split into train/test sets.\n\n\nThe target variable Y (what should be predicted)\n\\[\nY =\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\ny_3\n\\end{bmatrix}\n\\]\n\n\nThe input data (what predicts)\n\\[\nX =\n\\begin{bmatrix}\n1 & \\text{Age}_1 & \\text{Sex}_1 \\\\\n1 & \\text{Age}_2 & \\text{Sex}_2 \\\\\n1 & \\text{Age}_3 & \\text{Sex}_3\n\\end{bmatrix}\n\\]\n\n\n3. Parameters (Weights / Coefficients)\nThe tunable variables that define a specific instance of the model.\nExamples:\n\nw, b in linear regression\nConnection weights in a neural network\nDuring learning, these parameters are adjusted to best fit the data\n\n\\[\nw =\n\\begin{bmatrix}\nw_1 \\\\\nw_2 \\\\\nw_3\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "What is machine learning?",
      "Regression from scratch"
    ]
  },
  {
    "objectID": "Regression_from_scratch.html#detour-matrix-multiplication",
    "href": "Regression_from_scratch.html#detour-matrix-multiplication",
    "title": "Regression from scratch",
    "section": "Detour: Matrix multiplication",
    "text": "Detour: Matrix multiplication\nMatrix multiplication is one of the most common operations in linear algebra.\nWhen we multiply two matrices, the number of columns in the first must match the number of rows in the second.\nIf \\(\\mathbf{A}\\) is of shape \\(m \\times n\\) and \\(\\mathbf{B}\\) is of shape \\(n \\times p\\),\nthen the result \\(\\mathbf{C} = \\mathbf{A}\\mathbf{B}\\) will have shape \\(m \\times p\\).\nEach element of the result is the dot product of a row from \\(\\mathbf{A}\\) and a column from \\(\\mathbf{B}\\):\n\\[\nc_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj}\n\\]\nAs an example, here is the multiplication of a 2 x 3 by a 3 x 2 matrix, resulting in a 2 x 2 matrix:\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w} + \\mathbf{b} + \\boldsymbol{\\varepsilon} =\n\\]\n\\[\n=\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23}\n\\end{bmatrix}\n\\begin{bmatrix}\nb_{11} & b_{12} \\\\\nb_{21} & b_{22} \\\\\nb_{31} & b_{32}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\na_{11}b_{11} + a_{12}b_{21} + a_{13}b_{31} & a_{11}b_{12} + a_{12}b_{22} + a_{13}b_{32} \\\\\na_{21}b_{11} + a_{22}b_{21} + a_{23}b_{31} & a_{21}b_{12} + a_{22}b_{22} + a_{23}b_{32}\n\\end{bmatrix}\n\\]\nExercise:\n\nExercise: Linear Regression Prediction\nA linear regression model estimates the relationship between predictors (features) and a target variable.\nGiven an input matrix \\(\\mathbf{X}\\) and a weight vector \\(\\mathbf{w}\\), the model predicts target values \\(\\hat{\\mathbf{y}}\\) according to:\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w}\n\\]\n\nTask\nYou are given the following data X, that the following weights w were fitted to:\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n1 & 2 \\\\\n1 & 4 \\\\\n1 & 6\n\\end{bmatrix},\n\\quad\n\\mathbf{w} =\n\\begin{bmatrix}\n0.5 \\\\\n1.2\n\\end{bmatrix}\n\\]\n\nCompute the predicted values \\(\\hat{\\mathbf{y}}\\) (by hand).\nDo the same using the numpy package using the dot product:\n\nimport numpy as np\nX = np.array([[1,2],[1,4],[1,6]])\nw = np.array([[0.5],[1.2]])\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGoal: Compute \\(\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w}\\).\nGiven \\[\n\\mathbf{X} =\n\\begin{bmatrix}\n1 & 2 \\\\\n1 & 4 \\\\\n1 & 6\n\\end{bmatrix},\n\\quad\n\\mathbf{w} =\n\\begin{bmatrix}\n0.5 \\\\\n1.2\n\\end{bmatrix}\n\\]\nStep-by-step \\[\n\\hat{\\mathbf{y}} =\n\\begin{bmatrix}\n1 & 2 \\\\\n1 & 4 \\\\\n1 & 6\n\\end{bmatrix}\n\\begin{bmatrix}\n0.5 \\\\\n1.2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1\\cdot0.5 + 2\\cdot1.2 \\\\\n1\\cdot0.5 + 4\\cdot1.2 \\\\\n1\\cdot0.5 + 6\\cdot1.2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2.9 \\\\\n5.3 \\\\\n7.7\n\\end{bmatrix}\n\\]\nResult \\[\n\\hat{\\mathbf{y}} =\n\\begin{bmatrix}\n2.9 \\\\\n5.3 \\\\\n7.7\n\\end{bmatrix}\n\\]\n** Using NumPy check**\nimport numpy as np\nX = np.array([[1,2],[1,4],[1,6]])\nw = np.array([[0.5],[1.2]])\n#y_hat = X @ w\nnp.dot(X,w)\ny_hat\n\n\n\n\n\n\n4. Objective or Loss Function\nA quantitative measure of how well the model performs.\n\nIt defines the goal of learning.\nExamples:\n\nMean Squared Error (MSE) for regression\nCross-Entropy Loss for classification\nNegative log-likelihood for probabilistic models\n\n\nAim: The algorithm tries to minimize (or maximize) this loss function.\nExample: mean squared loss \\[\nL = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2\n\\]\n\\(L\\) — total loss (the value we minimize)\n\\(n\\) — number of samples\n\\(y_i\\) — true (observed) value for sample i\n\\(\\hat{y}_i\\) — predicted value for sample i\nThe squared difference \\((\\hat{y}_i - y_i)^2\\) measures the error for each prediction. Taking the mean makes the loss independent of sample size.\n\nTask\nYou are given a target vector \\(y\\), in addition to the previous vector \\(\\hat{y}\\):\n\\[\n\\mathbf{y} =\n\\begin{bmatrix}\n3.0 \\\\\n5.0 \\\\\n8.0\n\\end{bmatrix},\n\\qquad\n\\hat{\\mathbf{y}} =\n\\begin{bmatrix}\n2.9 \\\\\n5.3 \\\\\n7.7\n\\end{bmatrix}\n\\]\n\nCalculate the mean square loss between the predicted changes from the previous task and the target vector y by hand.\nWrite a function in python to do it. The function should take \\(y\\) and \\(\\hat{y}\\) as input and return the mean squared loss:\n\nLoss = mean_quared_loss(y, y_hat)\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can compute the Mean Squared Error (MSE) step by step.\nGiven\n\\[\n\\mathbf{y} =\n\\begin{bmatrix}\n3.0 \\\\\n5.0 \\\\\n8.0\n\\end{bmatrix},\n\\qquad\n\\hat{\\mathbf{y}} =\n\\begin{bmatrix}\n2.9 \\\\\n5.3 \\\\\n7.7\n\\end{bmatrix}\n\\]\n\n\nStep 1 — Write the formula\n\\[\nL = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2\n\\]\nHere \\(n = 3\\).\n\n\n\nStep 2 — Compute the individual errors\n\n\n\n\n\n\n\n\n\n\n(i)\n(y_i)\n(_i)\n(_i - y_i)\n((_i - y_i)^2)\n\n\n\n\n1\n3.0\n2.9\n-0.1\n0.01\n\n\n2\n5.0\n5.3\n+0.3\n0.09\n\n\n3\n8.0\n7.7\n-0.3\n0.09\n\n\n\n\n\n\nStep 3 — Sum and average\n\\[\nL = \\frac{1}{3} (0.01 + 0.09 + 0.09)\n= \\frac{0.19}{3}\n= 0.0633\n\\]\nFinal Result \\[\nL = 0.0633\n\\]\n\n\n\n\n\n\n\n\n\n\n\nSolution Python\n\n\n\n\n\nimport numpy as np\n\ny = np.array([3.0, 5.0, 8.0])\ny_hat = np.array([2.9, 5.3, 7.7])\n\ndef mse(y, y_hat):\n    result = np.mean((y_hat - y)**2)\n    return result\n\n\n\n\n\n\n5. Optimization Algorithm (Learning Rule)\nThe procedure that adjusts the parameters to minimize the loss.\nExamples: - Gradient Descent and its variants (SGD, Adam) - Expectation-Maximization (EM)\n\n\nGradient descent based on the loss function\nWhen fitting the w, we want to minimize the loss function. Let’s go step by step:\n\n\nGradient of Linear Regression\nWe start with the linear regression model for a single sample (i):\n\\[\n\\hat{y}_i = w x_i + b\n\\]\nFor \\(n\\) samples, the mean squared error (MSE) loss is:\n\\[\nL = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2\n\\]\nSubstituting the model prediction gives us the function we need to minimize:\n\\[\nL = \\frac{1}{n} \\sum_{i=1}^n (w x_i + b - y_i)^2\n\\]\nTo minimize the loss function, we need to calculate the derivatives with respect to \\(w\\) (slope parameter in this case) and b (intercept parameter in this case).\n\nCalculate the gradient (first derivative) with respect to \\(w\\) (slope).\nCalculate the gradient (first derivative) with respect to \\(b\\) (intercept).\n\nTips: Use differential calculus. You can treat the sum like a bracket ( ). Don’t forget the chain rule, if applicable.\n\n\n\n\n\n\nSolution Gradients\n\n\n\n\n\n\n1 Gradient with respect to \\(w\\):\n\\[\n\\frac{\\partial L}{\\partial w}\n= \\frac{1}{n} \\sum_{i=1}^n 2 (w x_i + b - y_i) x_i\n= \\frac{2}{n} \\sum_{i=1}^n (w x_i + b - y_i) x_i\n\\]\n\n\n2 Gradient with respect to \\(b\\):\n\\[\n\\frac{\\partial L}{\\partial b}\n= \\frac{1}{n} \\sum_{i=1}^n 2 (w x_i + b - y_i)\n= \\frac{2}{n} \\sum_{i=1}^n (w x_i + b - y_i)\n\\]\n\n\n\n\n\n\n6 Learning rule\nThe learning rule \\(\\eta\\) (“eta”) helps to regulate updating the all parameters.\n\\[\\theta := \\theta - \\eta \\frac{\\partial L}{\\partial \\theta}\\]\nWe run this updating algorithm in a loop, spanning 1000 iterations (“epochs”). Finally, we have to select a random set of starting parameters.\n\n\nTask: Putting it all together.\nWe are now ready to write the full algorithm.\n\nStart with a new data set, for example age and cortical thickness, generated from this code:\n\n\nimport numpy as np\nn = 100\nage = np.linspace(0, 1, n)  # age, standardized\n\n# Generate target: cortical thickness (in mm)\n# slightly decreasing with age + noise\nthickness = 2.8 - 0.008 * age + np.random.normal(0, 0.05, n)\n\n\nInitialize your weights, w and b, each at starting point 0.0\nFit b and w, with a learning rate of 1e-2 and 1000 iterations using python.\n\n\n\n\n\n\n\nSolution Regression\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Training data\nX = age      # input feature (age, for example)\ny = thickness  # target (e.g., cortical thickness)\nn = len(X)\n\n# Initialize parameters\nw = 0.0\nb = 0.0\neta = 1e-2       # learning rate\nepochs = 2000    # number of iterations\n\n# Gradient descent loop\nfor epoch in range(epochs):\n    y_hat = w * X + b\n    dw = (2/n) * np.sum((y_hat - y) * X)\n    db = (2/n) * np.sum(y_hat - y)\n    \n    # update weights\n    w -= eta * dw\n    b -= eta * db\n    \n    # print progress occasionally\n    if epoch % 200 == 0:\n        loss = np.mean((y_hat - y)**2)\n        print(f\"Epoch {epoch:4d}: w={w:.3f}, b={b:.3f}, Loss={loss:.4f}\")\n\n# Final weights\nprint(f\"Final weights: w={w:.3f}, b={b:.3f}\")\n\n\nplt.figure(figsize=(7,5))\nplt.scatter(age, thickness, color=\"black\", label=\"Data (observed)\", alpha=0.7)\nplt.plot(age, b + w * age, color=\"red\", linewidth=2, label=\"Fitted regression line\")\nplt.xlabel(\"Age (years)\")\nplt.ylabel(\"Cortical thickness (mm)\")\nplt.title(\"Linear Regression Fit via Gradient Descent\")\nplt.legend()\nplt.grid(True, linestyle=\"--\", alpha=0.6)\nplt.savefig(\"imgs/linear_regression_fit.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\nEpoch    0: w=0.028, b=0.056, Loss=7.8512\nEpoch  200: w=0.874, b=2.316, Loss=0.0755\nEpoch  400: w=0.664, b=2.444, Loss=0.0449\nEpoch  600: w=0.499, b=2.533, Loss=0.0271\nEpoch  800: w=0.372, b=2.601, Loss=0.0167\nEpoch 1000: w=0.275, b=2.653, Loss=0.0106\nEpoch 1200: w=0.201, b=2.692, Loss=0.0070\nEpoch 1400: w=0.145, b=2.723, Loss=0.0050\nEpoch 1600: w=0.102, b=2.746, Loss=0.0037\nEpoch 1800: w=0.068, b=2.764, Loss=0.0030\nFinal weights: w=0.043, b=2.777",
    "crumbs": [
      "What is machine learning?",
      "Regression from scratch"
    ]
  },
  {
    "objectID": "types-of-ml.html",
    "href": "types-of-ml.html",
    "title": "Types of Machine learning Algorithms",
    "section": "",
    "text": "Use when: target is numeric (e.g., gait speed, length of stay, brain-age delta).\n\nLinear Regression — baseline linear fit; fast, interpretable.\n\nRidge (L2) — shrinks coefficients; helps multicollinearity.\n\nLasso (L1) — feature selection via sparsity.\n\nElastic Net — hybrid of L1/L2; robust when features are correlated.\n\nGeneralized Linear Models (GLM) — non-Gaussian outcomes (Poisson, Gamma).\n\nSupport Vector Regression (SVR) — robust, kernelized nonlinearity.\n\nTree Ensembles (RF/GBM) — capture nonlinearity & interactions.\n\nUse-cases: predict 6MWT, therapy hours, WMH volume change, LOS.\n\n\n\nUse when: target is categorical (e.g., fall/no-fall, discharge home vs facility).\n\nLogistic Regression — strong baseline, well-calibrated.\nNaive Bayes — simple probabilistic baseline (text, high-dim data).\nk-Nearest Neighbors (kNN) — instance-based, local decision boundaries.\nDecision Trees / Random Forests — interpretable (trees) / strong default (RF).\nGradient Boosted Trees — XGBoost / LightGBM / CatBoost; top tabular performers.\nSupport Vector Machines (SVM) — good in high-dimensional settings.\n\nUse-cases: classify stroke discharge, fall risk from IMU, diagnosis subtypes."
  },
  {
    "objectID": "types-of-ml.html#supervised-learning",
    "href": "types-of-ml.html#supervised-learning",
    "title": "Types of Machine learning Algorithms",
    "section": "",
    "text": "Use when: target is numeric (e.g., gait speed, length of stay, brain-age delta).\n\nLinear Regression — baseline linear fit; fast, interpretable.\n\nRidge (L2) — shrinks coefficients; helps multicollinearity.\n\nLasso (L1) — feature selection via sparsity.\n\nElastic Net — hybrid of L1/L2; robust when features are correlated.\n\nGeneralized Linear Models (GLM) — non-Gaussian outcomes (Poisson, Gamma).\n\nSupport Vector Regression (SVR) — robust, kernelized nonlinearity.\n\nTree Ensembles (RF/GBM) — capture nonlinearity & interactions.\n\nUse-cases: predict 6MWT, therapy hours, WMH volume change, LOS.\n\n\n\nUse when: target is categorical (e.g., fall/no-fall, discharge home vs facility).\n\nLogistic Regression — strong baseline, well-calibrated.\nNaive Bayes — simple probabilistic baseline (text, high-dim data).\nk-Nearest Neighbors (kNN) — instance-based, local decision boundaries.\nDecision Trees / Random Forests — interpretable (trees) / strong default (RF).\nGradient Boosted Trees — XGBoost / LightGBM / CatBoost; top tabular performers.\nSupport Vector Machines (SVM) — good in high-dimensional settings.\n\nUse-cases: classify stroke discharge, fall risk from IMU, diagnosis subtypes."
  },
  {
    "objectID": "types-of-ml.html#unsupervised-learning",
    "href": "types-of-ml.html#unsupervised-learning",
    "title": "Types of Machine learning Algorithms",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\n\nClustering (find structure in unlabeled data)\n\nk-Means — spherical clusters; fast; needs k.\nHierarchical clustering — dendrogram; no fixed k.\nDBSCAN — density-based; finds arbitrary shapes; flags noise.\nGaussian Mixture Models (GMM) — soft assignments; probabilistic.\n\nUse-cases: patient subtyping, gait pattern families, sensor-based phenotypes."
  },
  {
    "objectID": "types-of-ml.html#dimensionality-reduction-feature-extraction",
    "href": "types-of-ml.html#dimensionality-reduction-feature-extraction",
    "title": "Types of Machine learning Algorithms",
    "section": "Dimensionality reduction / Feature extraction",
    "text": "Dimensionality reduction / Feature extraction\n\nPCA — orthogonal components maximizing variance.\nICA — source separation (EEG/fMRI).\nt-SNE / UMAP — nonlinear embeddings (visualization).\nAutoencoders — learned low-dim representations.\nFeature selection — Lasso, RFE, mutual information.\n\nUse-cases: reduce voxel/ROI features, compress IMU time-series features."
  },
  {
    "objectID": "types-of-ml.html#anomaly-novelty-detection",
    "href": "types-of-ml.html#anomaly-novelty-detection",
    "title": "Types of Machine learning Algorithms",
    "section": "Anomaly / Novelty detection",
    "text": "Anomaly / Novelty detection\n\nGaussian Mixtures — density modeling; low-probability = anomaly.\nOne-Class SVM — boundary around “normal”.\nIsolation Forest — isolates rare points via random splits.\nLocal Outlier Factor (LOF) — local density deviations.\n\nUse-cases: flag sensor glitches, unusual recovery trajectories, QC in pipelines."
  },
  {
    "objectID": "types-of-ml.html#probabilistic-time-series",
    "href": "types-of-ml.html#probabilistic-time-series",
    "title": "Types of Machine learning Algorithms",
    "section": "Probabilistic & time-series",
    "text": "Probabilistic & time-series\n\nHidden Markov Models (HMM) — discrete latent states for sequences (e.g., gait phases).\nGaussian Processes (GP) — Bayesian nonparametric regression/classification with uncertainty.\nBayesian Networks — causal/conditional dependencies among variables.\n\nUse-cases: session-to-session recovery trajectories, gait state decoding, uncertainty."
  },
  {
    "objectID": "types-of-ml.html#ensembles",
    "href": "types-of-ml.html#ensembles",
    "title": "Types of Machine learning Algorithms",
    "section": "Ensembles",
    "text": "Ensembles\nEnsemble methods\n\nBagging — reduces variance (e.g., Random Forest).\nBoosting — reduces bias with additive trees (e.g., XGBoost, LightGBM, CatBoost).\nStacking — meta-learner combining diverse base models.\n\nUse-cases: robust tabular prediction with mixed clinical + sensor features."
  },
  {
    "objectID": "types-of-ml.html#optional-deep-learning",
    "href": "types-of-ml.html#optional-deep-learning",
    "title": "Types of Machine learning Algorithms",
    "section": "(Optional) Deep learning",
    "text": "(Optional) Deep learning\nNeural networks (when data are large / structured)\n\nMLP — general nonlinear function approximator for tabular + small signals.\nCNN — spatial/temporal patterns (images, spectrograms, kinematic grids).\nRNN/GRU/LSTM — sequence modeling (time-series).\nTransformers — state-of-the-art for sequences; scalable attention.\n\nUse-cases: MRI/US imaging, multi-sensor sequences, language notes (NLP)."
  },
  {
    "objectID": "types-of-ml.html#choosing-a-model-fast-decision-guide",
    "href": "types-of-ml.html#choosing-a-model-fast-decision-guide",
    "title": "Types of Machine learning Algorithms",
    "section": "Choosing a model (fast decision guide)",
    "text": "Choosing a model (fast decision guide)\n\nSmall/medium tabular data: start with Logistic/Linear, Random Forest, Gradient Boosting.\n\nHigh-dimensional & few samples: try Lasso/Elastic Net, SVM, RF (use strong CV).\n\nUnlabeled exploration: PCA/UMAP + clustering (DBSCAN/GMM).\n\nTime-series / stateful: HMM, RNN/Transformer (if data are sufficient).\n\nNeed calibrated probabilities: Logistic Regression, CalibratedClassifierCV.\n\n\n\n\n\n\n\nWarning\n\n\n\nValidation matters more than model choice.\nUse stratified/nested CV, GroupKFold for repeated patients, temporal splits for prognosis, and check calibration + subgroup fairness."
  },
  {
    "objectID": "types-of-ml.html#key-sklearn-entry-points",
    "href": "types-of-ml.html#key-sklearn-entry-points",
    "title": "Types of Machine learning Algorithms",
    "section": "Key sklearn entry points",
    "text": "Key sklearn entry points\n\nLinearRegression, Ridge, Lasso, ElasticNet, PoissonRegressor\n\nLogisticRegression, SVC/SVR, KNeighbors*, GaussianNB\n\nDecisionTree*, RandomForest*, GradientBoosting*\n\nKMeans, AgglomerativeClustering, DBSCAN, GaussianMixture\n\nPCA, TruncatedSVD, FactorAnalysis, SelectKBest, RFE\n\nOneClassSVM, IsolationForest, LocalOutlierFactor\n\nPipelines & Validation: Pipeline, ColumnTransformer, StratifiedKFold, GroupKFold, GridSearchCV, CalibratedClassifierCV"
  },
  {
    "objectID": "types-of-ml.html#further-reading",
    "href": "types-of-ml.html#further-reading",
    "title": "Types of Machine learning Algorithms",
    "section": "Further reading",
    "text": "Further reading\n\nscikit-learn User Guide (models, pipelines, model evaluation)\n\nElements of Statistical Learning (Hastie, Tibshirani, Friedman)\n\nInterpretable ML (Molnar) — SHAP, partial dependence, global vs local explanations"
  },
  {
    "objectID": "Clustering_from_scratch.html",
    "href": "Clustering_from_scratch.html",
    "title": "Clustering from scratch",
    "section": "",
    "text": "The aim of k-means clustering is to group a set of data points into k distinct clusters based on their similarity, such that:\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import k_means\nimport matplotlib.pyplot as plt\ncenters = [[1, 1], [-1, -1], [1, -1]]\ndata, labels_true = make_blobs(\n    n_samples=750, centers=centers, cluster_std=0.4, random_state=0\n)\n\ndata = StandardScaler().fit_transform(data) # make sure we have clusters that are of roughly equal size \n\nprint(data) \ndata.shape\n\n[[ 0.49426097  1.45106697]\n [-1.42808099 -0.83706377]\n [ 0.33855918  1.03875871]\n ...\n [-0.05713876 -0.90926105]\n [-1.16939407  0.03959692]\n [ 0.26322951 -0.92649949]]\n\n\n(750, 2)\nfig,ax = plt.subplots()\nax.scatter(data[:,0], data[:,1])\nplt.show()\nThe plots clearly shows 3 clusters. Our aim is to identify these three clusters from the image.",
    "crumbs": [
      "What is machine learning?",
      "Clustering from scratch"
    ]
  },
  {
    "objectID": "Clustering_from_scratch.html#assigning-points-to-the-nearest-centroids.",
    "href": "Clustering_from_scratch.html#assigning-points-to-the-nearest-centroids.",
    "title": "Clustering from scratch",
    "section": "2. Assigning points to the nearest centroids.",
    "text": "2. Assigning points to the nearest centroids.\nNow that we have our initial guesses for the centroids, we can go to step 2.\nWe will run this step in two nested loops. The aim of this step is to assign each point from data to the nearest cluster centroid.\nThe first loop runs trough the rows of data, selecting one row each. In a second, inner loop, we calculate the distance of the point defined by that row from the three centroids.\n\nEuclidean distance between two points\nBut first we need a measure to evaluate the distance between two data points.\nWe use the euclidean distance between two vectors \\(\\mathbf{x}, \\mathbf{y}\\):\n\\[d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\\]\n\n\nTask: Euclidean distance\nWrite a function that calculates the euclidean distance between two data points (which are defined by an x and a y coordinate). Run the function to calculate the euclidean distance between the first row of data and the first row in centroids\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef calculate_euclidean_distance(point1, point2):\n    return np.sqrt(np.sum((point1 - point2) ** 2))\n\ncalculate_euclidean_distance(data[0], centroids[0])\n\n2.509460051330634\n\n\n\n\n\n\n\nTask Inner loop\nWrite a loop to compare the first row of data to all three rows of centroids. Compare the outcomes. Save (for example in a list) the centroid that is the closest to the point defined by the row in data.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nresults =[]\nfor i, entry in enumerate(centroids):\n    distance = calculate_euclidean_distance(data[0], entry)\n    results.append(distance)\n\nclosest_index = np.argmin(results)\n\nprint(closest_index)\n\n2\n\n\n\n\n\n\n\nTask: Outer loop\nNow put this loop into an outer loop, iterating over data.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nclosest_index = []\n\nfor j, row in enumerate(data):\n    results = []\n    for i, entry in enumerate(centroids):\n        distance = calculate_euclidean_distance(row, entry)\n        results.append(distance)\n\n    # Append the index of the centroid closest to this data point\n    closest_index.append(np.argmin(results))\n\nprint(closest_index)\n\n[2, 1, 2, 0, 2, 0, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 0, 0, 0, 0, 0, 2, 2, 1, 0, 0, 2, 2, 2, 2, 0, 1, 2, 0, 2, 2, 2, 2, 0, 2, 1, 1, 1, 1, 0, 2, 2, 2, 1, 2, 2, 0, 1, 2, 2, 0, 2, 0, 1, 0, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 0, 2, 0, 1, 0, 2, 2, 2, 2, 1, 2, 1, 0, 0, 2, 1, 2, 0, 2, 0, 1, 2, 2, 0, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0, 1, 2, 2, 0, 2, 1, 0, 0, 2, 2, 1, 2, 1, 2, 2, 2, 2, 0, 0, 0, 2, 1, 0, 0, 2, 1, 2, 2, 2, 2, 0, 0, 0, 2, 1, 2, 0, 1, 0, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 0, 1, 0, 0, 2, 0, 2, 1, 2, 2, 0, 1, 0, 0, 2, 2, 1, 0, 2, 2, 0, 2, 2, 1, 2, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 0, 2, 2, 2, 1, 2, 1, 0, 0, 2, 1, 0, 2, 1, 2, 0, 1, 0, 0, 0, 2, 2, 0, 1, 1, 2, 1, 2, 2, 2, 0, 2, 2, 0, 2, 1, 0, 2, 2, 1, 0, 2, 2, 2, 0, 2, 2, 0, 0, 2, 1, 0, 2, 2, 0, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 0, 0, 2, 2, 2, 2, 0, 2, 2, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 0, 2, 2, 0, 1, 0, 2, 0, 2, 1, 0, 2, 2, 2, 2, 2, 1, 1, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 0, 0, 1, 1, 0, 2, 1, 2, 2, 1, 0, 2, 2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 0, 0, 0, 1, 2, 1, 0, 2, 0, 0, 2, 0, 2, 1, 1, 2, 0, 0, 1, 0, 1, 2, 2, 0, 1, 0, 0, 2, 0, 2, 1, 1, 2, 0, 0, 2, 0, 2, 2, 2, 0, 0, 2, 0, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 0, 2, 2, 2, 1, 2, 0, 1, 2, 1, 2, 2, 2, 0, 0, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 0, 0, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 0, 2, 1, 2, 0, 2, 2, 1, 0, 0, 2, 2, 2, 2, 1, 2, 0, 1, 2, 2, 0, 1, 2, 2, 2, 2, 1, 0, 2, 2, 2, 2, 1, 0, 0, 1, 2, 1, 2, 1, 2, 2, 0, 2, 1, 0, 0, 0, 1, 2, 0, 2, 0, 1, 2, 2, 1, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 1, 0, 0, 0, 2, 2, 2, 1, 0, 2, 2, 2, 2, 0, 0, 2, 0, 1, 2, 2, 1, 2, 0, 2, 2, 2, 2, 1, 1, 0, 2, 2, 2, 0, 2, 0, 0, 2, 0, 2, 1, 1, 0, 0, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 2, 2, 1, 0, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 1, 1, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 0, 0, 2, 0, 2, 2, 1, 0, 2, 1, 2, 2, 2, 1, 0, 2, 2, 2, 0, 2, 2, 2, 1, 0, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 0, 0, 2, 0, 1, 1, 1, 2, 0, 2, 2, 2, 0, 0, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 0, 2, 0, 0, 2, 0, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 0, 2, 1, 2, 2, 0, 2, 2, 1, 1, 0, 1, 2, 1, 0, 0, 0]\n\n\n\n\n\n\n\nAssigning the points.\nNow that we have assigned all points in data to the nearest centroid, we can color the points based on their assignment to the nearest cluster.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nfig,ax = plt.subplots()\nscatter = ax.scatter(data[:, 0], data[:, 1], c=closest_index, cmap='viridis', alpha=0.7)\nax.scatter(centroids[:, 0], centroids[:, 1], color='red', s=100, marker='X', label='Centroids')\nplt.savefig(\"imgs/cluster_assignment.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Calculate new mean per cluster\nWe have now assigned all points to these first 3 clusters. Now we calculate the new cluster mean per cluster.\nFirst, we add the vector containing the assignment to the cluster to the data frame.\n\ndata = np.column_stack((data, closest_index)) # add closest_index as column\n\nThen we calculate the new means of those clusters.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Calculate the new_centroids\n\nk = 3  # Number of clusters\nnew_centroids =[]\nfor i in range(k):\n    #print(i)\n    # Mask data belonging to cluster i\n    cluster_points = data[data[:,2]==float(i)]\n    centroid = cluster_points[:, :2].mean(axis=0)\n    # Compute mean of these points\n    new_centroids.append(centroid)\nprint(new_centroids)\n\n[array([-0.31769297, -0.58065271]), array([-1.344061  , -0.91313225]), array([0.7280849 , 0.70601105])]\n\n\n\n\n\n\nPlot the new centroids\nwe can now plot the new centroids, by updaing the first plot.\n\nnew_centroids = np.array(new_centroids)  # Convert list to (k, 2) NumPy array\n\nfig, ax = plt.subplots()\nscatter = ax.scatter(data[:, 0], data[:, 1], c=closest_index, cmap='viridis', alpha=0.7)\n\n# Now this works\nax.scatter(new_centroids[:, 0], new_centroids[:, 1], color='red', s=100, marker='X', label='Centroids')\n\nplt.savefig(\"imgs/cluster_assignment2.png\")\nplt.show()",
    "crumbs": [
      "What is machine learning?",
      "Clustering from scratch"
    ]
  },
  {
    "objectID": "Clustering_from_scratch.html#repeat.-reassign-the-points.",
    "href": "Clustering_from_scratch.html#repeat.-reassign-the-points.",
    "title": "Clustering from scratch",
    "section": "Repeat. Reassign the points.",
    "text": "Repeat. Reassign the points.\nWe now do steps 3 and 4, until the centroids are stable.",
    "crumbs": [
      "What is machine learning?",
      "Clustering from scratch"
    ]
  },
  {
    "objectID": "Clustering_from_scratch.html#putting-it-all-together",
    "href": "Clustering_from_scratch.html#putting-it-all-together",
    "title": "Clustering from scratch",
    "section": "Putting it all together:",
    "text": "Putting it all together:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# --- Settings ---\nk = 3\nmax_iters = 10  # Keep small for now\n\n# --- Use only x,y coordinates (keep it simple) ---\nX = data[:, :2]  # if data has 2 cols already, this is identical\nn = X.shape[0]\n\n# --- 1) Initialize centroids by sampling rows from data ---\nindices = random.sample(range(n), k)\ncentroids = X[indices, :].copy()\n\n# --- Distance function (your version) ---\ndef calculate_euclidean_distance(point1, point2):\n    return np.sqrt(np.sum((point1 - point2) ** 2))\n\n# Plot helper\ndef plot_clusters(X, labels, centroids, i):\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.7)\n    ax.scatter(centroids[:, 0], centroids[:, 1], color='red', s=120, marker='X', label='Centroids')\n    ax.set_title(f\"K-means: iteration {i+1}\")\n    ax.legend()\n    plt.colorbar(scatter, ax=ax, label=\"Centroid index\")\n    plt.show()\n\n# --- 2) Iterate: assign -&gt; update -&gt; plot ---\nfor it in range(max_iters):\n    # Assign each point to nearest centroid (your nested-loop style)\n    closest_index = []\n    for j, row in enumerate(X):\n        distances = [calculate_euclidean_distance(row, c) for c in centroids]\n        closest_index.append(int(np.argmin(distances)))\n    closest_index = np.array(closest_index, dtype=int)\n\n    # Plot current state\n    plot_clusters(X, closest_index, centroids, it)\n\n    # Update centroids\n    new_centroids = []\n    for i in range(k):\n        cluster_points = X[closest_index == i]\n        if cluster_points.size &gt; 0:\n            centroid = cluster_points.mean(axis=0)\n        else:\n            # reinit empty cluster to a random point\n            centroid = X[random.randrange(n)]\n        new_centroids.append(centroid)\n    new_centroids = np.array(new_centroids)\n\n    # Stop if nothing changes\n    if np.allclose(new_centroids, centroids):\n        print(f\"Converged after {it+1} iteration(s).\")\n        break\n\n    centroids = new_centroids\n\nprint(f\"Finished after {it+1} iteration(s).\")\nprint(\"Final centroids:\\n\", centroids)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConverged after 5 iteration(s).\nFinished after 5 iteration(s).\nFinal centroids:\n [[ 0.6954587  -0.64442334]\n [ 0.62260555  1.3172598 ]\n [-1.30266211 -0.65704205]]",
    "crumbs": [
      "What is machine learning?",
      "Clustering from scratch"
    ]
  },
  {
    "objectID": "Clustering_from_scratch.html#evaluation-metrics",
    "href": "Clustering_from_scratch.html#evaluation-metrics",
    "title": "Clustering from scratch",
    "section": "Evaluation metrics",
    "text": "Evaluation metrics\nWe have now decided to cluster into 3 clusters. But how do we know that is this the right amount of clusters? We can use a measure called inertia, which is the summed distance from all points to their respective cluster centroid:\n\\[\\text{Inertia} = \\sum_{i=1}^{k} \\sum_{\\mathbf{x} \\in C_i} \\|\\mathbf{x} - \\boldsymbol{\\mu}_i\\|^2\\]\nwhere:\n•   $C_i$ is the set of points belonging to cluster i\n\n•   $\\boldsymbol{\\mu}_i$ is the centroid of cluster i",
    "crumbs": [
      "What is machine learning?",
      "Clustering from scratch"
    ]
  },
  {
    "objectID": "Clustering_from_scratch.html#scikit-learn",
    "href": "Clustering_from_scratch.html#scikit-learn",
    "title": "Clustering from scratch",
    "section": "Scikit-learn",
    "text": "Scikit-learn\nFirst, we make the clustering a bit easier. The scikit-learn package allows clustering with a single line of code.\n\n1. Install scikit-learn into your .venv\npip install -U scikit-learn\n\n\n2. Clustering with scikit-learn\nWe repeat the clustering approach from above with one line of code:\n\ncentroid, label, inertia = k_means(\n    data, n_clusters=3, n_init=\"auto\", random_state=0\n)\n\nAs we can see, the function returns three results: centroids (the location), labels (the assignments) and inertia.\nWe can now easily loop over this function to find the optimal amount of clusters, by comparing inertia. We then plot the inertia for each loop in an elbow plot:\n\nelbow = list() # list to store interias\nfor k in range(9):\n    centroid, label, inertia = k_means(\n            data, n_clusters=k+1, n_init=\"auto\", random_state=0\n    )\n    elbow.append(inertia)\n\n\n\n3. Generate elbow plot.\nLet’s generate an elbow plot to plot the inertia:\n\nplt.plot(range(1, 10), elbow, marker='o')\nplt.title('Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()",
    "crumbs": [
      "What is machine learning?",
      "Clustering from scratch"
    ]
  },
  {
    "objectID": "task_setup.html",
    "href": "task_setup.html",
    "title": "Task: Setting up VS Code for Python",
    "section": "",
    "text": "Set up and test your Python development environment in VS Code using a virtual environment.",
    "crumbs": [
      "Set-up",
      "Task: Setting up VS Code for Python"
    ]
  },
  {
    "objectID": "task_setup.html#objective",
    "href": "task_setup.html#objective",
    "title": "Task: Setting up VS Code for Python",
    "section": "",
    "text": "Set up and test your Python development environment in VS Code using a virtual environment.",
    "crumbs": [
      "Set-up",
      "Task: Setting up VS Code for Python"
    ]
  },
  {
    "objectID": "task_setup.html#step-by-step-instructions",
    "href": "task_setup.html#step-by-step-instructions",
    "title": "Task: Setting up VS Code for Python",
    "section": "Step-by-step instructions",
    "text": "Step-by-step instructions\n\n1. Check your Python installation\nOpen your terminal and verify that Python is installed: bash    python3 --version\n\n\n2. Install VS Code extensions\nOpen VS Code and install the following: • Python extension • Jupyter extension\n\n\n3. Create and activate a virtual environment\nIn the terminal (inside your project folder), run:\npython3 -m venv .venv\nsource .venv/bin/activate       # macOS/Linux\nOR\n.\\.venv\\Scripts\\Activate.ps1    # Windows\nThen install a few essential packages:\npip install jupyter numpy scikit-learn matplotlib pandas\n\n\n4. Test Python and NumPy in the terminal\nLaunch Python from within your activated environment and check:\nimport numpy as np\nprint(np.__version__)\n\n\n5. Test in VS Code using a Python script\n•   Create a new file called test.py\n•   Add the following code:\nimport numpy as np\nprint(\"NumPy works!\", np.__version__)\n\n\n6. Test in a Jupyter Notebook\n•   Create a new notebook named test.ipynb\n•   In the first cell, add:\nimport numpy as np\nnp.random.rand(3)\n• Run the cell and confirm it executes successfully.\n\n\n7. (Optional) Try running in Google Colab\nUpload your test.ipynb to Google Colab. In the first cell, install NumPy and run:\n!pip install numpy\nimport numpy as np\nprint(\"NumPy works in Colab!\", np.__version__)",
    "crumbs": [
      "Set-up",
      "Task: Setting up VS Code for Python"
    ]
  },
  {
    "objectID": "basic-jupyter.html",
    "href": "basic-jupyter.html",
    "title": "Jupyter Notebooks in Google Colab",
    "section": "",
    "text": "Alternatively, if you want to use Python without worrying about installing libraries or managing dependencies, you can use Jupyter notebooks and run them directly in Google Colab — a free, cloud-based platform provided by Google.",
    "crumbs": [
      "Set-up",
      "Jupyter Notebooks in Google Colab"
    ]
  },
  {
    "objectID": "basic-jupyter.html#what-is-google-colab",
    "href": "basic-jupyter.html#what-is-google-colab",
    "title": "Jupyter Notebooks in Google Colab",
    "section": "1. What is Google Colab?",
    "text": "1. What is Google Colab?\nGoogle Colab (short for Collaboratory) is a cloud service that lets you run Jupyter notebooks in your browser, with no local setup required.\nKey advantages: - No need to install Python or libraries. - Access to free GPUs and TPUs for computation. - Direct integration with Google Drive for storage. - Same syntax and workflow as local Jupyter notebooks (.ipynb files).\nYou can think of it as “Jupyter Notebook in the cloud.”",
    "crumbs": [
      "Set-up",
      "Jupyter Notebooks in Google Colab"
    ]
  },
  {
    "objectID": "basic-jupyter.html#how-to-open-a-notebook-in-colab",
    "href": "basic-jupyter.html#how-to-open-a-notebook-in-colab",
    "title": "Jupyter Notebooks in Google Colab",
    "section": "2. How to open a notebook in Colab",
    "text": "2. How to open a notebook in Colab\n\nOption A – Start from scratch\n\nGo to https://colab.research.google.com\n\nChoose File → New notebook.\n\nA notebook with a Python 3 runtime will open in your browser.\n\nYou can rename it and save it to your Google Drive.\n\n\n\nOption B – Open an existing notebook\nIf you already have a .ipynb file:\n\nUpload it to Google Drive.\n\nRight-click → Open with → Google Colaboratory.\n(If Colab isn’t listed, choose Connect more apps → search for Colaboratory.)\n\nAlternatively, you can drag-and-drop a notebook directly into an open Colab window.",
    "crumbs": [
      "Set-up",
      "Jupyter Notebooks in Google Colab"
    ]
  },
  {
    "objectID": "basic-jupyter.html#installing-additional-libraries",
    "href": "basic-jupyter.html#installing-additional-libraries",
    "title": "Jupyter Notebooks in Google Colab",
    "section": "3. Installing additional libraries",
    "text": "3. Installing additional libraries\nColab comes pre-installed with most scientific packages (NumPy, pandas, matplotlib, scikit-learn, PyTorch, TensorFlow, etc.).\nTo install any additional packages, use pip directly in a cell:\n!pip install seaborn xgboost shap",
    "crumbs": [
      "Set-up",
      "Jupyter Notebooks in Google Colab"
    ]
  },
  {
    "objectID": "ml-overview.html",
    "href": "ml-overview.html",
    "title": "What is machine learning?",
    "section": "",
    "text": "Machine learning and classical statistics look at different questions on different levels (group, individual) and use different methods.",
    "crumbs": [
      "What is machine learning?",
      "What is machine learning?"
    ]
  },
  {
    "objectID": "ml-overview.html#different-types-of-machine-learning",
    "href": "ml-overview.html#different-types-of-machine-learning",
    "title": "What is machine learning?",
    "section": "Different types of machine learning",
    "text": "Different types of machine learning\n\n\n\n\n\n\n\nSupervised Machine Learning\nUnsupervised Machine Learning\n\n\n\n\nInput: X, Y  Output: Patterns or associations between X and Y  Examples:  - Regression of body weight (Y) onto age (X)  - Classification of bird species (Y) based on body weight (X)\nInput: Y  Output: Patterns derived from Y following a set of rules  Example:  - Subgroups within bird species based on characteristics of the beak (Y)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nRule of thumb: start simple (linear/logistic, trees), validate rigorously, then consider more complex models if they add clear value.",
    "crumbs": [
      "What is machine learning?",
      "What is machine learning?"
    ]
  },
  {
    "objectID": "ml-overview.html#concepts-in-machine-learning",
    "href": "ml-overview.html#concepts-in-machine-learning",
    "title": "What is machine learning?",
    "section": "Concepts in Machine learning",
    "text": "Concepts in Machine learning\n\nCross-validation\n\nCross-validation is a technique used to evaluate how well a machine learning model generalizes to new, unseen data.\nThe data is split into several parts (called folds). The model is trained on some folds and tested on the remaining one — this process is repeated so that every fold serves once as a test set.\nThe average performance across all folds gives a more reliable estimate of the model’s accuracy or error than a single train–test split.\nIn short: train multiple times, test multiple times, average the results → estimate generalization.\n\n\n\n\nGrid search\n\nGrid search is a method used to find the best set of hyperparameters for a machine learning model.\nIt systematically tries all possible combinations of parameter values from a predefined “grid,” trains a model for each combination (often using cross-validation), and compares their performance.\nThe combination that gives the best validation score is selected as the optimal set of parameters.\nIn short: try many parameter combinations → evaluate → keep the best one.\n\n\n\nLoss function\n\nA loss function measures how far a model’s predictions are from the true values — it’s what the model tries to minimize during training.\nWhile grid search optimizes hyperparameters (external settings like learning rate or regularization strength), the loss function guides the optimization of the model parameters themselves (e.g., weights and intercepts).\nIn short: the loss function teaches the model, whereas grid search tunes how the model learns.",
    "crumbs": [
      "What is machine learning?",
      "What is machine learning?"
    ]
  },
  {
    "objectID": "ml-overview.html#optimizer",
    "href": "ml-overview.html#optimizer",
    "title": "What is machine learning?",
    "section": "Optimizer",
    "text": "Optimizer\n\nAn optimizer is the algorithm that automatically adjusts the model’s parameters (like weights and intercepts) to minimize the loss function.\nIt’s how the model learns from data.\nAt every step, the optimizer looks at how changing each parameter would affect the loss, and then moves them in the direction that reduces the loss the most.\n\nExamples of optimizers\n\nGradient Descent — the basic algorithm that updates parameters step by step in the direction of the steepest decrease of loss.\nStochastic Gradient Descent (SGD) — updates using small random batches of data (faster for big datasets).\nAdam, RMSProp, Adagrad — advanced optimizers that adapt learning rates automatically for each parameter.",
    "crumbs": [
      "What is machine learning?",
      "What is machine learning?"
    ]
  }
]